---
title: "Easily Build Customized LLMs"
#filters:
  #- gradio
date: "2025-06-03"
format:
  html:
    toc: true
    theme: cosmo  # Or another theme you like
image: images/LLM_Fables.jpg
embed-resources: true
resources:
  - "images"
filters:
  - gradio
gradio:
  cdn: "https://peter-gy.github.io/static/npm/gradio-lite"
  attributes:
    playground: true
execute:
  echo: false
  warning: false
  output: true
  freeze: true 
  #engine: jupyter
categories: [LLMs, Python, OpenAI, Hugging Face]
#jupyter: python3

---

# Introduction

Customized LLMs can be created to enable you to have full control of your own propiertary PDFs or text files stored locally with just a few lines of code. Journalists who are always on deadliine, can respond quickly to new regulations or court hearing decisions speeding up the time to digest the new material and be the first to respond without risk of exposing their material to the world.

Deployed on Hugging Face the Private LLM contains documents of publicly available Fables from around the world. With a simple interface, you can query about Aesop or Grimms Fairy tales. Query fables that exists in other parts of the world. Of course this is a demo and the documents are public, but if you clone the LLM demo, then run it locally - you will have your own localized LLM. This LLM is ideal if you want to combine copyrighted material that the larger LLMs may not have been trained on. You may have clients that are risk adverse about having any propietary documents even on a secured server. It's also great as a demo if you would like to show what LLMs can do, and most importantly what their limitations are.

# Source


```{python}
##| gr-playground: true
##| gr-layout: vertical
#import gradio as gr
```

Setting up python

```{bash}
#source venv/bin/activate #this sets up isolated python environment
```


Setting up reticulate


```{r setup, include=FALSE}
#install.packages("reticulate")
#library(reticulate)
#use_virtualenv("godata")
```


```{r}
#Sys.getenv("RETICULATE_PYTHON")

#library(reticulate, quietly = T)

#suppressPackageStartupMessages(library("reticulate"))
##options(reticulate.repl.quiet = TRUE)
##use_virtualenv("godata")
#store python environment in .Renviron in project directory

#use_python("/usr/local/bin/python3")
#Sys.getenv("RETICULATE_PYTHON")
```


Going to try using pyenv godata

```{r}
#library(reticulate)
#reticulate::py_config()
```




Python packages required

```{bash}

# Install core components
pip3 install llama-index-core 
pip3 install python-dotenv 
pip3 install duckdb 
pip3 install gradio

# Install integrations
pip3 install llama-index-llms-openai
pip3 install llama-index-embeddings-openai
pip3 install llama_index.vector_stores.duckdb

# Install integrations
pip3 install llama-index-llms-openai
pip3 install llama-index-embeddings-openai
pip3 install dotenv

```

```{python}

import duckdb
from dotenv import load_dotenv
import os
import openai
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
import textwrap
import gradio as gr

```

This LLM creates a vector store, that we need to make sure is deleted from any previous runs.

```{python}

file_path = 'persist/my_vector_store.duckdb'

# Check if file exists
if os.path.exists(file_path):
  #Delete the file
  os.remove(file_path)
  print("File deleted successfully")
else:
  print("File doesn't exist - first run - it's all good")

```

Next we go get the openai key, if we don't already have one and set up the environment so that we are able to access the openai indexing capability using llama indexing provided by Meta open source.

```{python}
from dotenv import load_dotenv
import os

# Point to the secrets/.env file
load_dotenv(dotenv_path="secrets/.env")

# Get the API key
api_key = os.getenv("OPENAI_API_KEY")

if api_key is None:
    raise ValueError("API key not found in secrets/.env")

print("API key loaded successfully!")  # Do not print actual key in production

```

Here we import the indexing packages to store the indexing in DuckDB.

```{python}

from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.vector_stores.duckdb import DuckDBVectorStore
from llama_index.core import StorageContext

vector_store = DuckDBVectorStore("my_vector_store.duckdb", persist_dir="persist/")
documents = SimpleDirectoryReader("/Users/Eileen/Desktop/GoData/Blog/posts/LLM_Demo/PDFs/").load_data()

```

This is where storage_context points to your indexed PDFs in the storage you specified above

```{python}
storage_context = StorageContext.from_defaults(vector_store=vector_store)
index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)
```

# Deployment

This is a basic interface so that you can query anything about the PDFs you stored.

```{python}
#| label: my-chunk
#| eval: true
##| include: true
##| echo: true
##| gr-theme: light

import gradio as gr
import os

#GRADIO_SERVER_PORT = int(os.environ.get("GRADIO_SERVER_PORT", 7860))
#GRADIO_NUM_PORTS = int(os.environ.get("GRADIO_NUM_PORTS",10))

#import gradio as gr

# Create a custom theme with blue as the primary color
#theme = gr.themes.Default()  
#theme = gr.themes.Default(primary_hue="blue", font=["Helvetica", "sans-serif"])

def greet(query):
    #print("Before query")  # Debugging
    query_engine = index.as_query_engine()
    response = query_engine.query(query)
    #print("After query")   # Debugging
    return str(response)

gr.Interface(
  fn=greet,
  inputs=gr.Textbox(lines=1, placeholder="Enter your query here...",
  label="Your Query"),
  outputs=gr.Textbox(label="Response")).launch()


```

```{python}
# Create a custom theme with blue as the primary color
#theme = gr.themes.Default()  


#def greet(query):
    
    ##query_engine = index.as_query_engine()
    ##response = query_engine.query(query)
    ##strresponse = str(response)
    #return(gradio.Markdown(strresponse))
    #return(textwrap.fill(str(response), 80))
    ##return(f"{response}")
    #display(Markdown(f"<b>{response}</b>")
    #return "Hello " + query + "!"

#demo = gr.Interface(fn=greet, inputs="text", outputs="text")
#demo.launch(share=False)   
```




