{
  "hash": "62cc93943f932c0eee2502cf2ebb094b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Easily Build Private Small LLMs\"\nimage: llm_fables.jpg\ndate: \"2025-06-03\"\ncategories: [LLMs, Python, OpenAI, Hugging Face]\n#jupyter: python3\n#filters:\n # - gradio\n---\n\n\n\n# Introduction\n\nPrivate LLMs can be created to enable you to have full control of your own propiertary PDFs or text files stored locally with just a few lines of code. Journalists who are always on deadliine, can respond quickly to new regulations or court hearing decisions speeding up the time to digest the new material and be the first to respond without risk of exposing their material to the world.\n\nDeployed on Hugging Face the Private LLM contains documents of publicly available Fables from around the world. With a simple interface, you can query about Aesop or Grimms Fairy tales. Query fables that exists in other parts of the world. Of course this is a demo and the documents are public, but if you clone the LLM demo, then run it locally - you will have your own localized LLM. This LLM is ideal if you want to combine copyrighted material that the larger LLMs may not have been trained on. You may have clients that are risk adverse about having any propietary documents even on a secured server. It's also great as a demo if you would like to show what LLMs can do, and most importantly what their limitations are.\n\nSo, lets get started:\n\nSetting up reticulate\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reticulate, quietly = T)\n#suppressPackageStartupMessages(library(\"reticulate\"))\noptions(reticulate.repl.quiet = TRUE)\n#use_virtualenv(\"r-reticulate\")\n#use_python(\"/usr/local/bin/python3\")\n```\n:::\n\n\n\nGoing to try using pyenv go_llm\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#library(reticulate)\nreticulate::py_config()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\npython:         /Users/Eileen/.virtualenvs/godata/bin/python\nlibpython:      /Users/Eileen/.pyenv/versions/3.12.6/lib/libpython3.12.dylib\npythonhome:     /Users/Eileen/.virtualenvs/godata:/Users/Eileen/.virtualenvs/godata\nvirtualenv:     /Users/Eileen/.virtualenvs/godata/bin/activate_this.py\nversion:        3.12.6 (main, Sep 26 2024, 09:29:53) [Clang 15.0.0 (clang-1500.3.9.4)]\nnumpy:          /Users/Eileen/.virtualenvs/godata/lib/python3.12/site-packages/numpy\nnumpy_version:  2.2.2\n\nNOTE: Python version was forced by VIRTUAL_ENV\n```\n\n\n:::\n:::\n\n\n\nPython packages required\n\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n\n# Install core components\n#pip3 install llama-index-core \n#pip3 install python-dotenv \n#pip3 install duckdb \n#pip3 install gradio\n\n# Install integrations\n#pip3 install llama-index-llms-openai\n#pip3 install llama-index-embeddings-openai\n#pip3 install llama_index.vector_stores.duckdb\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n\n# Install integrations\n#pip install llama-index-llms-openai\n#pip install llama-index-embeddings-openai\n#pip install dotenv\n\nimport duckdb\nfrom dotenv import load_dotenv\nimport os\nimport openai\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nimport textwrap\nimport gradio as gr\n```\n:::\n\n\n\nThis LLM creates a vector store, that we need to make sure is deleted from any previous runs.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfile_path = 'persist/my_vector_store.duckdb'\n\n# Check if file exists\nif os.path.exists(file_path):\n  #Delete the file\n  os.remove(file_path)\n  print(\"File deleted successfully\")\nelse:\n  print(\"File doesn't exist - first run - it's all good\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFile deleted successfully\n```\n\n\n:::\n:::\n\n\n\nNext we go get the openai key, if we don't already have one and set up the environment so that we are able to access the openai indexing capability using llama indexing provided by Meta open source.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()  # Loads variables from .env\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue\n```\n\n\n:::\n\n```{.python .cell-code}\napi_key = os.getenv(\"OPENAI_API_KEY\")\n```\n:::\n\n\n\nHere we import the indexing packages to store the indexing in DuckDB.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.vector_stores.duckdb import DuckDBVectorStore\nfrom llama_index.core import StorageContext\n\nvector_store = DuckDBVectorStore(\"my_vector_store.duckdb\", persist_dir=\"./persist/\")\ndocuments = SimpleDirectoryReader(\"/Users/Eileen/Desktop/GoData/Blog/posts/LLM_Demo/PDFs/\").load_data()\n```\n:::\n\n\n\nThis is where storage_context points to your indexed PDFs in the storage you specified above\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\nindex = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n```\n:::\n\n\n\nThis is a basic interface so that you can query anything about the PDFs you stored.\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Create a custom theme with blue as the primary color\ntheme = gr.themes.Default()  \n\n\ndef greet(query):\n    \n    query_engine = index.as_query_engine()\n    response = query_engine.query(query)\n    strresponse = str(response)\n    #return(gradio.Markdown(strresponse))\n    #return(textwrap.fill(str(response), 80))\n    return(f\"{response}\")\n    #display(Markdown(f\"<b>{response}</b>\")\n    #return \"Hello \" + query + \"!\"\n\n#demo = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\n#demo.launch(share=True)   \n```\n:::\n\n\n\nThis is output, you can now query the public PDFs here on Hugging Face to see how it works. This output is similar but not generated from the code above. There is a seperate script generating the LLM on Hugging Face that you can access by cloning the repository at https://huggingface.co/spaces/GoData/Fables-beta/tree/main.\n\nThis is from: https://huggingface.co/spaces/GoData/Fables-beta\n\n\n\n```{=html}\n<iframe width=\"780\" height=\"500\" src=\"https://godata-fables-beta.hf.space/\" title=\"Quarto Documentation\"></iframe>\n```\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}