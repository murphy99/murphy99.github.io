[
  {
    "objectID": "posts/Shinylive/index.html",
    "href": "posts/Shinylive/index.html",
    "title": "Converting Shiny Apps to ShinyLive",
    "section": "",
    "text": "Shiny Apps - can be converted to Shinylive using Quarto in just a few lines of code. If the Shiny app isn’t too complex, we can provide a serverless interactive experience using just an html website. The Shiny App that we converted is one of Posit’s simple Shiny Apps in its gallery(Posit 2014). ShinyLive allows a more frictionless way to share your app without having to use a hosted server by running on its own html page served to the client. The html is rendered in quarto to automatically download the needed assets to the client and it can run from their own pc or mac.\nThe Shiny App that we chose is the KMeans model using the Iris dataset, provides a way to convert a Shiny App, but it’s also a handy tool to deploy on our website. There were just 2 things we had to change and that was using the {shinylive-r} instead of {r} and to change ui to using fluidPage from the bslib library to ensure app resizes well to different devices. You’ll notice that older shiny apps don’t resize well, and this is why."
  },
  {
    "objectID": "posts/Shinylive/index.html#introduction",
    "href": "posts/Shinylive/index.html#introduction",
    "title": "Converting Shiny Apps to ShinyLive",
    "section": "",
    "text": "Shiny Apps - can be converted to Shinylive using Quarto in just a few lines of code. If the Shiny app isn’t too complex, we can provide a serverless interactive experience using just an html website. The Shiny App that we converted is one of Posit’s simple Shiny Apps in its gallery(Posit 2014). ShinyLive allows a more frictionless way to share your app without having to use a hosted server by running on its own html page served to the client. The html is rendered in quarto to automatically download the needed assets to the client and it can run from their own pc or mac.\nThe Shiny App that we chose is the KMeans model using the Iris dataset, provides a way to convert a Shiny App, but it’s also a handy tool to deploy on our website. There were just 2 things we had to change and that was using the {shinylive-r} instead of {r} and to change ui to using fluidPage from the bslib library to ensure app resizes well to different devices. You’ll notice that older shiny apps don’t resize well, and this is why."
  },
  {
    "objectID": "posts/Shinylive/index.html#kmeans",
    "href": "posts/Shinylive/index.html#kmeans",
    "title": "Converting Shiny Apps to ShinyLive",
    "section": "KMeans",
    "text": "KMeans\nKMeans is an unsupervised machine learning algorithm used to group similar data together without already predefined classifications. All the points are clustered around a centroid that represents the mean of all points within the cluster. The number of centroids are referred to as “K values” and represents the number groups within all the data ponts. The goal is to minimize the euclidean distance between the data point and the its centroid mean, maximize the separation between clusters, and minimize total squared distance between each point and it’s assigned centriod.\nIn order to find the optimal K values, you need to run through and iterative process. This KMeans model uses Shinylive to be able to iterate quickly between K values to visually see the best grouping or at least to narrow down the K values to calculate the optimal number of categories.\n\n#install.packages(\"shiny\")\n#install.packages(\"shinylive\")\nlibrary(shiny)\nlibrary(shinylive)\nlibrary(bslib)\n\n\nAttaching package: 'bslib'\n\n\nThe following object is masked from 'package:utils':\n\n    page\n\n\n\nUpdate Quarto Extension\nRun this in terminal: quarto add quarto-ext/shinylive\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 500\n\n\nvars &lt;- setdiff(names(iris), \"Species\")\n\nui &lt;- fluidPage(\n  titlePanel('Iris k-means clustering'),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput('xcol', 'X Variable', vars),\n      selectInput('ycol', 'Y Variable', vars, selected = vars[[2]]),\n      numericInput('clusters', 'Cluster count', 3, min = 1, max = 9)\n    ),\n    mainPanel(\n      plotOutput('plot1')\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  selectedData &lt;- reactive({\n    iris[, c(input$xcol, input$ycol)]\n  })\n  clusters &lt;- reactive({\n    kmeans(selectedData(), input$clusters)\n  })\n  output$plot1 &lt;- renderPlot({\n    palette(c(\"#E41A1C\", \"#377EB8\", \"#4DAF4A\", \"#984EA3\",\n              \"#FF7F00\", \"#FFFF33\", \"#A65628\", \"#F781BF\", \"#999999\"))\n    par(mar = c(5.1, 4.1, 0, 1))\n    plot(selectedData(),\n         col = clusters()$cluster,\n         pch = 20, cex = 3)\n    points(clusters()$centers, pch = 4, cex = 4, lwd = 4)\n  })\n}\n\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "posts/Shinylive/index.html#deployment",
    "href": "posts/Shinylive/index.html#deployment",
    "title": "Converting Shiny Apps to ShinyLive",
    "section": "Deployment",
    "text": "Deployment\nWe can take a shiny app and convert it to shiny live that enables the application to run on just the client’s browser from an html page.\n\nInstall required packages\n\n\nVerify you can retrieve the shinylive assets\n\nshinylive::assets_info()\n\nshinylive R package version: 0.3.0\n\n\nshinylive web assets version: 0.9.1\n\n\n\n\n\nLocal cached shinylive asset dir:\n\n\n→ '/Users/Eileen/Library/Caches/shinylive'\n\n\n\n\n\nInstalled assets:\n\n\n• '/Users/Eileen/Library/Caches/shinylive/shinylive-0.9.1'\n\n\n\n\nYAML (Remove comments and add to your Yaml) :\n#— #format: #html: #embed-resources: false #filters: #- shinylive #—"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GoData.ca",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEasily Build Customized LLMs\n\n\n\nPython\n\nOPENAI\n\nLLMs\n\nJupyter\n\nOBBBA\n\n\n\n\n\n\n\n\n\nJul 15, 2025\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\nConverting Shiny Apps to ShinyLive\n\n\n\nR\n\nShinylive\n\nKMeans\n\n\n\n\n\n\n\n\n\nJun 30, 2025\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\nRSS Feeds\n\n\n\nR\n\nJavascript\n\ntidyRSS\n\n\n\n\n\n\n\n\n\nJun 15, 2025\n\n2 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/RSS_Reader/index0.html",
    "href": "posts/RSS_Reader/index0.html",
    "title": "RSS Feeds",
    "section": "",
    "text": "RSS Readers may not be in fashion but I believe they are on the comeback. Inspired by an article from InfoWorld(Machlis 2022), with some slight modifications, I created an rss feed to list many the research publications. The list of research papers can look overwhelming, but can be refined or filtered using the search boxes.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\nlibrary(DT)\nlibrary(purrr)\nlibrary(stringr)\nlibrary(lubridate)\nlibrary(tidyRSS)\n\n\nflag=0\n\nmy_feed &lt;- \"https://rss.arxiv.org/rss/cs.LG\"\n\nresult &lt;- tryCatch(\n  {\n    my_feed_data &lt;- tidyfeed(my_feed)\n    cat(\"Number of items:\", nrow(my_feed_data), \"\\n\")\n    my_feed_data |&gt; \n      select(feed_pub_date, item_title, item_link, item_description)\n  },\n  error = function(e) {\n    cat(\"Error caught:\", e$message, \"\\n\")\n    NULL  # return NULL on error\n  }\n)\n\nGET request successful. Parsing...\n\n\nError caught: subscript out of bounds \n\nif (is.null(result) || nrow(result) == 0) {\n  flag=1\n  cat(\"No papers published today\\n\")\n} else {\n  print(result)\n}\n\nNo papers published today\n\n\n\nif (! flag) {\n  my_feed_data_summary &lt;- my_feed_data |&gt;\n    select(item_title, feed_pub_date, item_link,item_description) }\n\n\nif (! flag) {\n#changed item_title to item_desc\nmy_rss_feed &lt;- my_feed_data_summary |&gt; mutate(\n    item_title = str_glue(\"&lt;a target='_blank' title='{item_title}' href='{item_link}' rel='noopener'&gt;{item_title}&lt;/a&gt;\")\n)}\n\n\nif (! flag) {\n  my_rss_feed_table &lt;- my_rss_feed |&gt; select(-item_link)}\n#my_feed_data_summary\n\n\n\nif (! flag) {\n  DT::datatable(my_rss_feed_table, filter = 'top', escape = FALSE, rownames = FALSE,\n    options = list(\n    search = list(regex = TRUE, caseInsensitive = TRUE),  \n    pageLength = 10,\n    lengthMenu = c(10, 25, 50, 100, 200),\n    autowidth = TRUE,\n   columnDefs = list(list(width = '80%', targets = list(2)))\n    )\n  )}\n\n\n\n\n\n\n\n\n\nReferences\n\nMachlis, Sharon. 2022. “How to Create Your Own RSS Reader with r _ InfoWorld.” InfoWorld, December. https://www.infoworld.com/article/2337738/how-to-create-your-own-rss-reader-with-r.html."
  },
  {
    "objectID": "posts/RSS_Reader/Most_Recent_Papers.html",
    "href": "posts/RSS_Reader/Most_Recent_Papers.html",
    "title": "Recent arXiv Research Papers (Updated M-F)",
    "section": "",
    "text": "GET request successful. Parsing...\n\n\nNumber of items: 179"
  },
  {
    "objectID": "posts/RSS_Reader/index.html",
    "href": "posts/RSS_Reader/index.html",
    "title": "RSS Feeds",
    "section": "",
    "text": "RSS Readers may not be in fashion but I believe they are on the comeback. Inspired by an article from InfoWorld(Machlis 2022), with some slight modifications, I created an rss feed to list many the research publications. The list of research papers can look overwhelming, but can be refined or filtered using the search boxes.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\nlibrary(DT)\nlibrary(purrr)\nlibrary(stringr)\nlibrary(lubridate)\nlibrary(tidyRSS)\n\n\nflag=0\n\nmy_feed &lt;- \"https://rss.arxiv.org/rss/cs.LG\"\n\nresult &lt;- tryCatch(\n  {\n    my_feed_data &lt;- tidyfeed(my_feed)\n    cat(\"Number of items:\", nrow(my_feed_data), \"\\n\")\n    my_feed_data |&gt; \n      select(feed_pub_date, item_title, item_link, item_description)\n  },\n  error = function(e) {\n    cat(\"Error caught:\", e$message, \"\\n\")\n    NULL  # return NULL on errorde\n  }\n)\n\nGET request successful. Parsing...\n\n\nNumber of items: 198 \n\nif (is.null(result) || nrow(result) == 0) {\n  flag=1\n  cat(\"Sorry, no papers published today\\n\")\n} else {\n  print(result)\n}\n\n# A tibble: 198 × 4\n   feed_pub_date       item_title                     item_link item_description\n   &lt;dttm&gt;              &lt;chr&gt;                          &lt;chr&gt;     &lt;chr&gt;           \n 1 2025-08-11 00:00:00 Diagrams-to-Dynamics (D2D): E… https://… arXiv:2508.0565…\n 2 2025-08-11 00:00:00 A Graph Neural Network Approa… https://… arXiv:2508.0572…\n 3 2025-08-11 00:00:00 Machine Learning-Based Nonlin… https://… arXiv:2508.0577…\n 4 2025-08-11 00:00:00 From Imperfect Signals to Tru… https://… arXiv:2508.0579…\n 5 2025-08-11 00:00:00 Optimal Linear Baseline Model… https://… arXiv:2508.0583…\n 6 2025-08-11 00:00:00 An Effective Approach for Nod… https://… arXiv:2508.0583…\n 7 2025-08-11 00:00:00 A Markov Decision Process Fra… https://… arXiv:2508.0587…\n 8 2025-08-11 00:00:00 The Fourth State: Signed-Zero… https://… arXiv:2508.0590…\n 9 2025-08-11 00:00:00 Dual Signal Decomposition of … https://… arXiv:2508.0591…\n10 2025-08-11 00:00:00 Fast, Convex and Conditioned … https://… arXiv:2508.0592…\n# ℹ 188 more rows\n\n\n\nif (! flag) {\n  my_feed_data_summary &lt;- my_feed_data |&gt;\n    select(item_title, feed_pub_date, item_link,item_description) }\n\n\nif (! flag) {\n#changed item_title to item_desc\nmy_rss_feed &lt;- my_feed_data_summary |&gt; mutate(\n    item_title = str_glue(\"&lt;a target='_blank' title='{item_title}' href='{item_link}' rel='noopener'&gt;{item_title}&lt;/a&gt;\")\n)}\n\n\nif (! flag) {\n  my_rss_feed_table &lt;- my_rss_feed |&gt; select(-item_link)}\n#my_feed_data_summary\n\n\n\nif (! flag) {\n  DT::datatable(my_rss_feed_table, filter = 'top', escape = FALSE, rownames = FALSE,\n    options = list(\n    search = list(regex = TRUE, caseInsensitive = TRUE),  \n    pageLength = 10,\n    lengthMenu = c(10, 25, 50, 100, 200),\n    autowidth = TRUE,\n   columnDefs = list(list(width = '80%', targets = list(2)))\n    )\n  )}\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nMachlis, Sharon. 2022. “How to Create Your Own RSS Reader with r _ InfoWorld.” InfoWorld, December. https://www.infoworld.com/article/2337738/how-to-create-your-own-rss-reader-with-r.html."
  },
  {
    "objectID": "walkthrough.html",
    "href": "walkthrough.html",
    "title": "Hello, Quarto",
    "section": "",
    "text": "Markdown is an easy to read and write text format:\n\nIt’s plain text so works well with version control\nIt can be rendered into HTML, PDF, and more\nLearn more at: https://quarto.org/docs/authoring/"
  },
  {
    "objectID": "walkthrough.html#markdown",
    "href": "walkthrough.html#markdown",
    "title": "Hello, Quarto",
    "section": "",
    "text": "Markdown is an easy to read and write text format:\n\nIt’s plain text so works well with version control\nIt can be rendered into HTML, PDF, and more\nLearn more at: https://quarto.org/docs/authoring/"
  },
  {
    "objectID": "walkthrough.html#code-cell",
    "href": "walkthrough.html#code-cell",
    "title": "Hello, Quarto",
    "section": "Code Cell",
    "text": "Code Cell\nHere is a Python code cell:\n\nimport os\nos.cpu_count()\n\n8"
  },
  {
    "objectID": "walkthrough.html#equation",
    "href": "walkthrough.html#equation",
    "title": "Hello, Quarto",
    "section": "Equation",
    "text": "Equation\nUse LaTeX to write equations:\n\\[\n\\chi' = \\sum_{i=1}^n k_i s_i^2\n\\]"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Eileen P. Murphy",
    "section": "",
    "text": "This blog centers on the exploration of data discovery and practical data science for implementing the use of machine learning, dashboards, LLMs, LLM Apps by using existing open source tools such as but not limited to R, Python, DuckDB, SQLite, MongoDB, Hadoop, and Spark.\nGoData.ca mission is to explore and replicate already available research and implement actual use cases for implementation.\nEileen Murphy is the owner and contributor to GoData.ca blog."
  },
  {
    "objectID": "about.html#about-the-blog",
    "href": "about.html#about-the-blog",
    "title": "Eileen P. Murphy",
    "section": "",
    "text": "This blog centers on the exploration of data discovery and practical data science for implementing the use of machine learning, dashboards, LLMs, LLM Apps by using existing open source tools such as but not limited to R, Python, DuckDB, SQLite, MongoDB, Hadoop, and Spark.\nGoData.ca mission is to explore and replicate already available research and implement actual use cases for implementation.\nEileen Murphy is the owner and contributor to GoData.ca blog."
  },
  {
    "objectID": "posts/LLM_Demo/index.html",
    "href": "posts/LLM_Demo/index.html",
    "title": "Easily Build Customized LLMs",
    "section": "",
    "text": "Whether you refer to the One Big Beautiful Bill or the One Big Bad Bill - this LLM demonstrates how to query the bill that is now law. Just how many unexposed nuggets buried in this bill is anyone’s guess. In the this case, it’s a useful way to query it. It does a pretty good job and you can see how you structure your query makes a difference on the response. Some prompts will be better than others. We listed some prompts below to get you started.\nOne of the use cases for localized LLMs is to help digest new bills, policies, and or court decisions in an efficient and expedient manner. Journalists frequently encounter brand new material before it becomes public and cannot store the document on any server.\nFor best results, we have found that adding other analysis improves the responses and contexts, but here we are just going to have the OBBA summary from the government website as a demonstration on how this can be used and test it to see if it would be comprehensive enough for journalists to use.\nIt’s important to check to make sure the responses are accurate. The 2 source documents are listed below. Including the section number in the query will make verifying the information easier - since you can just look it up.\nThis model uses minimally trained model from OPENAI, and fairly inexpensive of all the models to do the query. Since the subject matter is so narrow - we do not need a big model - just big enough to be fairly responsive to our queries.\nAdding more PDFs will get a richer and more comprehensive output. Working on your queries or prompts will also improve the results.\n\n\n\n#Install in packages (pip) in terminal - if missing\n#!pip install python-dotenv\nfrom dotenv import load_dotenv\n#pip install duckdb\nimport duckdb\n#pip install llama_index_core\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nimport os #built in package\n#pip install openai\nimport openai\n#pip install textwrap\nimport textwrap \n#pip install llama_index.vector_stores.duckdb\nimport llama_index.vector_stores.duckdb\n#pip install llama-index-embeddings-openai\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n#pip install llama-index-llms-openai\nfrom llama_index.llms.openai import OpenAI\n#pip install gradio\nimport gradio as gr\n\nThe LLM creates a vector store everytime it runs. So we delete it before storing the new vector\n\nfile_path = 'persist/my_vector_store.duckdb'\n\n# Check if file exists\nif os.path.exists(file_path):\n  #Delete the file\n  os.remove(file_path)\n  print(\"File deleted successfully\")\nelse:\n  print(\"File doesn't exist - first run - it's all good\")\n\nFile deleted successfully\n\n\nNext we find the OPENAI key and set up the environment so that we are able to use its index capability and using llama indexing provided by Meta’s open source\n\nfrom dotenv import load_dotenv\n#load_dotenv()\n# Point to the secrets/.env file\nload_dotenv(dotenv_path=\"secrets/.env\")\n\napi_key = os.getenv('OPENAI_API_KEY')\n\n\nfrom openai import OpenAI\nclient = OpenAI(api_key=api_key)\n\nHere we import the indexing packages to store the indexing in DuckDB.\n\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.vector_stores.duckdb import DuckDBVectorStore\nfrom llama_index.core import StorageContext\n\n\nvector_store = DuckDBVectorStore(\"my_vector_store.duckdb\", persist_dir=\"persist/\")\ndocuments = SimpleDirectoryReader(input_dir=\"OBBBA/\").load_data()\n\nThis is where storage_context points to your indexed PDFs in the storage you specified above\n\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\nindex = VectorStoreIndex.from_documents(documents, storage_context=storage_context)"
  },
  {
    "objectID": "posts/Welcome/index.html",
    "href": "posts/Welcome/index.html",
    "title": "Welcome",
    "section": "",
    "text": "This blog is using Quarto. The YAML and the structure of the directories were puzzling to me at first. The directory for each blog entry, YAML configuration files for global and local document settings. The seperation between source and rendered files is a game changer because you can have a zippity site rendered in just html and javascript and another layer of source files used to render and maintain the site. Once you get the hang of it, it’s fun to work on.\n\nQuarto, a publishing pipeline\nAs I was warming up to Quarto - I stumbled on was a book on how to self publish on Amazon using Quarto to generate from text to Kindle format. Software to replace Word? Now, my interest was piqued.\nBut, while impressive isn’t perfect quite yet. While Quarto has a Jupyter python kernal for your notebooks, some of the outputs such as Gridio sends output to the console instead of the terminal. While this is a flaw, I believe the fix - if it hasn’t been solved will be soon.\nIf you are Jupyter notebook and want them working with Quarto right away, Jeremy Howard has an excellent site to get you going with having the best both worlds of integrating Jupyter notebooks and the publishing bells and whistles used in Quarto.\nI could not ignore Quarto, any longer.\nhttps://www.fast.ai/posts/2022-08-25-jupyter-git.html https://www.fast.ai/posts/2022-07-28-nbdev2.html#our-new-secret-weapon-for-productivity\nhttps://nbdev.fast.ai/getting_started.html\nthink python"
  },
  {
    "objectID": "posts/RSS_Reader/Most_Recent_Papers.html#make-sure-that-are-papers-published-today",
    "href": "posts/RSS_Reader/Most_Recent_Papers.html#make-sure-that-are-papers-published-today",
    "title": "Recent arXiv Research Papers (Updated M-F)",
    "section": "",
    "text": "flag=0\n\nmy_feed &lt;- \"https://rss.arxiv.org/rss/cs.LG\"\n\nresult &lt;- tryCatch(\n  {\n    my_feed_data &lt;- tidyfeed(my_feed)\n    cat(\"Number of items:\", nrow(my_feed_data), \"\\n\")\n    my_feed_data |&gt; \n      select(feed_pub_date, item_title, item_link, item_description)\n  },\n  error = function(e) {\n    cat(\"Error caught:\", e$message, \"\\n\")\n    NULL  # return NULL on errorde\n  }\n)\n\nError caught: could not find function \"tidyfeed\" \n\nif (is.null(result) || nrow(result) == 0) {\n  flag=1\n  cat(\"Sorry, no papers published today\\n\")\n} else {\n  print(result)\n}\n\nSorry, no papers published today\n\n\n```"
  },
  {
    "objectID": "posts/RSS_Reader/Most_Recent_Papers.html#define-most-recent-function",
    "href": "posts/RSS_Reader/Most_Recent_Papers.html#define-most-recent-function",
    "title": "Recent arXiv Research Papers (Updated M-F)",
    "section": "Define most recent function",
    "text": "Define most recent function"
  },
  {
    "objectID": "posts/RSS_Reader/Most_Recent_Papers.html#make-sure-that-are-papers-in-the-pipeline-today",
    "href": "posts/RSS_Reader/Most_Recent_Papers.html#make-sure-that-are-papers-in-the-pipeline-today",
    "title": "Recent arXiv Research Papers (Updated M-F)",
    "section": "",
    "text": "GET request successful. Parsing...\n\n\nNumber of items: 179"
  },
  {
    "objectID": "posts/LLM_Demo/index.html#introduction",
    "href": "posts/LLM_Demo/index.html#introduction",
    "title": "Easily Build Customized LLMs",
    "section": "",
    "text": "Whether you refer to the One Big Beautiful Bill or the One Big Bad Bill - this LLM demonstrates how to query the bill that is now law. Just how many unexposed nuggets buried in this bill is anyone’s guess. In the this case, it’s a useful way to query it. It does a pretty good job and you can see how you structure your query makes a difference on the response. Some prompts will be better than others. We listed some prompts below to get you started.\nOne of the use cases for localized LLMs is to help digest new bills, policies, and or court decisions in an efficient and expedient manner. Journalists frequently encounter brand new material before it becomes public and cannot store the document on any server.\nFor best results, we have found that adding other analysis improves the responses and contexts, but here we are just going to have the OBBA summary from the government website as a demonstration on how this can be used and test it to see if it would be comprehensive enough for journalists to use.\nIt’s important to check to make sure the responses are accurate. The 2 source documents are listed below. Including the section number in the query will make verifying the information easier - since you can just look it up.\nThis model uses minimally trained model from OPENAI, and fairly inexpensive of all the models to do the query. Since the subject matter is so narrow - we do not need a big model - just big enough to be fairly responsive to our queries.\nAdding more PDFs will get a richer and more comprehensive output. Working on your queries or prompts will also improve the results.\n\n\n\n#Install in packages (pip) in terminal - if missing\n#!pip install python-dotenv\nfrom dotenv import load_dotenv\n#pip install duckdb\nimport duckdb\n#pip install llama_index_core\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nimport os #built in package\n#pip install openai\nimport openai\n#pip install textwrap\nimport textwrap \n#pip install llama_index.vector_stores.duckdb\nimport llama_index.vector_stores.duckdb\n#pip install llama-index-embeddings-openai\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n#pip install llama-index-llms-openai\nfrom llama_index.llms.openai import OpenAI\n#pip install gradio\nimport gradio as gr\n\nThe LLM creates a vector store everytime it runs. So we delete it before storing the new vector\n\nfile_path = 'persist/my_vector_store.duckdb'\n\n# Check if file exists\nif os.path.exists(file_path):\n  #Delete the file\n  os.remove(file_path)\n  print(\"File deleted successfully\")\nelse:\n  print(\"File doesn't exist - first run - it's all good\")\n\nFile deleted successfully\n\n\nNext we find the OPENAI key and set up the environment so that we are able to use its index capability and using llama indexing provided by Meta’s open source\n\nfrom dotenv import load_dotenv\n#load_dotenv()\n# Point to the secrets/.env file\nload_dotenv(dotenv_path=\"secrets/.env\")\n\napi_key = os.getenv('OPENAI_API_KEY')\n\n\nfrom openai import OpenAI\nclient = OpenAI(api_key=api_key)\n\nHere we import the indexing packages to store the indexing in DuckDB.\n\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.vector_stores.duckdb import DuckDBVectorStore\nfrom llama_index.core import StorageContext\n\n\nvector_store = DuckDBVectorStore(\"my_vector_store.duckdb\", persist_dir=\"persist/\")\ndocuments = SimpleDirectoryReader(input_dir=\"OBBBA/\").load_data()\n\nThis is where storage_context points to your indexed PDFs in the storage you specified above\n\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\nindex = VectorStoreIndex.from_documents(documents, storage_context=storage_context)"
  },
  {
    "objectID": "posts/LLM_Demo/index.html#deployment",
    "href": "posts/LLM_Demo/index.html#deployment",
    "title": "Easily Build Customized LLMs",
    "section": "Deployment",
    "text": "Deployment\nThis is a basic interface so that you can query anything about the PDF(s) you stored. In this case we stored two documents from https://www.congress.gov/:\nhttps://www.congress.gov/bill/119th-congress/house-bill/1/text\nhttps://www.congress.gov/bill/119th-congress/house-bill/1\nWe printed them to PDFs and stored them under the OBBBA directory.\nSome responses will not be what you expect and even provide inaccurate in incomplete responses. This is the nature of LLMs and is good to see it’s limitations. The prompts below have been tested and provide pretty accurate information from what I can see. The prompt area is live, so have fun while the funding lasts.\nUseful Prompts:\n“What is the summarization of OBBBA?”\n“What are the recissions, modifications and extensions of the programs and or organizations listed in the OBBBA? List by appearance with section number and title description. Output in list”\n“What are the SNAP and Medicaid of the programs and or organizations listed in the OBBBA? List by appearance with section number and title description. Output in LIst”\n“What is the DOJ fund named”BIDEN.” What is it used for? list by item. What provision is it under?“\n\nimport gradio as gr\n\n# Create a custom theme with blue as the primary color\ntheme = gr.themes.Default()  \n#theme = gr.themes.Default(primary_hue=\"blue\", font=[\"Helvetica\", \"sans-serif\"])\n\ndef greet(query):\n    query_engine = index.as_query_engine()\n    response = query_engine.query(query)\n    return str(response)\n\ngr.Interface(\n  fn=greet,\n  inputs=gr.Textbox(lines=1, placeholder=\"Enter you query here...\",\n  label=\"Your Query\"),\n  outputs=gr.Textbox(label=\"Response\")).launch(share=True, pwa=True)\n\n* Running on local URL:  http://127.0.0.1:7864\n* Running on public URL: https://7b2e0b9c17dd2f97fb.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)"
  }
]