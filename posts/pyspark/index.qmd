---
title: "Data Pipelines with Pyspark"
subtitle: "Using Canadian Anti-Fraud Centre Open Data"
date: "2025-08-15"
updated: "2025-10-15"
format:
  html:
    code-fold: true
    code-summary: "Show the code"
image: open_art_pipelines.png
html:
    code-fold: true
    include-after-body: add_button.html
categories: [Pyspark, Python, R, CAFC, Downloads]
draft: False

---

::: {.callout-note}
Updated 2025-10-15 Updated CAFC data with Q3 data published Oct 2, 2025.  
Improved automation to process files.
:::

## Introduction 

Pyspark is an invaluable tool to process datasets in parallel partitions for processing efficiency. The data can be written and merged to another file with many different options in format, such as csv, tsv, json, parquet, and xml. 

The structured pipeline once written will always be reproducible and easy to maintain.

The Canadian Anti-Fraud Centre updates and its data dictionary are available and updated every quarter on the CKAN platform on the [Open Government Portal](https://open.canada.ca/data/en/dataset/6a09c998-cddb-4a22-beff-4dca67ab892f/resource/43c67af5-e598-4a9b-a484-fe1cb5d775b5). The CKAN platform allows federal and municipal governments as well as companies to maintain their catalog of datasets in a consistent and transparent way, whether it's public or private to all their users.

## Process

In this exercise, we are going to use the Canadian Anti-Fraud Centre Reporting Dataset on the CKAN platform and do the following:  
1. Extract english and french field names into 2 datasets to represent English and French distinct but replicated datasets (in progress).  
2. Change date type to date and change fields to numeric that we want to aggregate.  
3. Include monthly and yearly aggregate datasets.  
4. Filter out invalid records that have invalid Country names.  
5. Streamline access to new generated datasets by downloading csv files to local drive.


### Function to run script to update datasets every quarter

```{python}
#| echo: true
# Don't run this here - put this in cronjob or github actions to update cafc dataset every quarter
#%run cafc_quarterly_fraud_data_pipeline.py #Set up on cronjob to run each quarter
```

### Contents of script to run to update datasets - only to be run once every quarter

```{r} 
#remove include (above) to generate lines of code used to extact cafc data
# get path to script
pathway <- here::here("posts","pyspark","cafc_quarterly_fraud_data_pipeline.py")

# generate output of lines from script

lines <- readLines(pathway, warn=FALSE)
cat(lines, sep = "\n")
```

### Required Libraries

```{r}
# Load necessary libraries
# install.packages("tidyverse")
suppressPackageStartupMessages(library(tidyverse))
```

### Find latest extracted cleaned cafc file

```{r}
# Clean (base) directory for posts
cafc_clean <- here::here("posts", "pyspark")

# Get a list of all output directories matching the pattern
csv_dir <- list.files(path = cafc_clean, pattern = "outputs_*", all.files = TRUE, full.names = TRUE)

# Get the most recent output directory (assuming sorted filenames)
extract_dir <- max(csv_dir)

# Concatenate the extracted directory and "cleaned_cafc" subdirectory correctly
cleaned_cafc_path <- file.path(extract_dir, "cleaned_cafc")

# Get all CSVs
file_paths <- list.files(path = cleaned_cafc_path, pattern = "\\.csv$", all.files = TRUE, full.names = TRUE)

```

### Merge the partitions to one file

```{r}
# Read and combine all CSV files into a single data frame
merged_df <- file_paths |>
  map_dfr(read_csv, show_col_types = FALSE)
```


### Show sample records of merged CAFC dataset
```{r}
# View the first few rows of the merged data frame
head(merged_df)
```

### Retrieve monthly summaries
```{r}

# Concatenate the extracted directory and "cleaned_cafc" subdirectory correctly
cleaned_monthly_summary_path <- file.path(extract_dir, "monthly_summary")

file_paths <- list.files(path = cleaned_monthly_summary_path, pattern = "\\.csv$", all.files = TRUE, full.names = TRUE)

monthly_summary_df <- file_paths %>%
  map_dfr(read_csv, show_col_types = FALSE)
```

### Show sample of monthly summary of victims
```{r}
head(monthly_summary_df)
```

### Retrieve yearly summaries of victims
```{r}
cleaned_yearly_summary_path <- file.path(extract_dir, "yearly_summary")


file_paths <- list.files(path = cleaned_yearly_summary_path, pattern = "\\.csv$", all.files = TRUE, full.names = TRUE)

yearly_summary_df <- file_paths %>%
  map_dfr(read_csv, show_col_types = FALSE)
```


### Show sample records of yearly summaries of victims
```{r}
head(yearly_summary_df)
```


## Deployment 

```{r}
library(stringr)

CAFC_latest_update <- str_sub(extract_dir, -7)
paste("CAFC Latest Update", CAFC_latest_update, sep= " : ")
```

### Download CSV files directly
```{r}
#| echo: true

library(downloadthis)

#Sys.setlocale("LC_NUMERIC", "C")
#options(OutDec = ".")

merged_df |>
  download_this(
    output_name = "clean_cafc_en",
    output_extension = ".csv",
    button_label = "Download clean_cafc_en",
    button_type = "default",
    self_contained = TRUE,
    has_icon = TRUE,
    icon = "fa fa-save",
    id = "cafc-btn",
    csv2 = FALSE      # Use write_csv(), dot decimal, comma separator
  )

```


```{r}
#| echo: true

library(downloadthis)

monthly_summary_df %>%
  download_this(
    output_name = "cafc_monthly_summary_en",
    output_extension = ".csv",
    button_label = "Download cafc_monthly_summary_en.csv",
    button_type = "default",
    self_contained = TRUE,
    has_icon = TRUE,
    icon = "fa fa-save",
    id = "cafc-btn",
    csv2 = FALSE 
  )
```


```{r}
#| echo: true

library(downloadthis)

yearly_summary_df %>%
  download_this(
    output_name = "cafc_yearly_summary_en",
    output_extension = ".csv",
    button_label = "Download cafc_yearly_summary_en.csv",
    button_type = "default",
    self_contained = TRUE,
    has_icon = TRUE,
    icon = "fa fa-save",
    id = "cafc-btn",
    csv2 = FALSE 
  )
```