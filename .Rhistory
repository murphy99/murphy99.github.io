# Install integrations
#pip install llama-index-llms-openai
#pip install llama-index-embeddings-openai
import duckdb
from dotenv import load_dotenv
import os
import openai
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
import textwrap
import gradio as gr
file_path = 'persist/my_vector_store.duckdb'
# Check if file exists
if os.path.exists(file_path):
#Delete the file
os.remove(file_path)
print("File deleted successfully")
else:
print("File doesn't exist - first run - it's all good")
from dotenv import load_dotenv
import os
load_dotenv()  # Loads variables from .env
api_key = os.getenv("OPENAI_API_KEY")
api_key
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.vector_stores.duckdb import DuckDBVectorStore
from llama_index.core import StorageContext
vector_store = DuckDBVectorStore("my_vector_store.duckdb", persist_dir="./persist/")
documents = SimpleDirectoryReader("/Users/Eileen/Desktop/GoData/Blog/posts/LLM_Demo/PDFs/").load_data()
storage_context = StorageContext.from_defaults(vector_store=vector_store)
index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)
#| label: my-chunk
#| eval: false
#| include: false
import gradio as gr
def greet(query):
print("Before query")  # Debugging
query_engine = index.as_query_engine()
response = query_engine.query(query)
print("After query")   # Debugging
return str(response)
gr.Interface(fn=greet,
inputs=gr.Textbox(lines=1, placeholder="Enter your query here..."),
outputs=gr.Textbox()).launch()
# Create a custom theme with blue as the primary color
theme = gr.themes.Default()
def greet(query):
query_engine = index.as_query_engine()
response = query_engine.query(query)
strresponse = str(response)
#return(gradio.Markdown(strresponse))
#return(textwrap.fill(str(response), 80))
return(f"{response}")
#display(Markdown(f"<b>{response}</b>")
#return "Hello " + query + "!"
#demo = gr.Interface(fn=greet, inputs="text", outputs="text")
#demo.launch(share=True)
# Install integrations
#pip install llama-index-llms-openai
#pip install llama-index-embeddings-openai
import duckdb
from dotenv import load_dotenv
import os
import openai
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
import textwrap
import gradio as gr
from dotenv import load_dotenv
import os
load_dotenv()  # Loads variables from .env
api_key = os.getenv("OPENAI_API_KEY")
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.vector_stores.duckdb import DuckDBVectorStore
from llama_index.core import StorageContext
vector_store = DuckDBVectorStore("my_vector_store.duckdb", persist_dir="./persist/")
documents = SimpleDirectoryReader("/Users/Eileen/Desktop/GoData/Blog/posts/LLM_Demo/PDFs/").load_data()
storage_context = StorageContext.from_defaults(vector_store=vector_store)
index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)
# Install integrations
#pip install llama-index-llms-openai
#pip install llama-index-embeddings-openai
import duckdb
from dotenv import load_dotenv
import os
import openai
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
import textwrap
import gradio as gr
file_path = 'persist/my_vector_store.duckdb'
# Check if file exists
if os.path.exists(file_path):
#Delete the file
os.remove(file_path)
print("File deleted successfully")
else:
print("File doesn't exist - first run - it's all good")
from dotenv import load_dotenv
import os
load_dotenv()  # Loads variables from .env
api_key = os.getenv("OPENAI_API_KEY")
from dotenv import load_dotenv
import os
load_dotenv()  # Loads variables from .env
api_key = os.getenv("OPENAI_API_KEY")
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.vector_stores.duckdb import DuckDBVectorStore
from llama_index.core import StorageContext
vector_store = DuckDBVectorStore("my_vector_store.duckdb", persist_dir="./persist/")
documents = SimpleDirectoryReader("/Users/Eileen/Desktop/GoData/Blog/posts/LLM_Demo/PDFs/").load_data()
storage_context = StorageContext.from_defaults(vector_store=vector_store)
index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)
# Create a custom theme with blue as the primary color
theme = gr.themes.Default()
def greet(query):
query_engine = index.as_query_engine()
response = query_engine.query(query)
strresponse = str(response)
#return(gradio.Markdown(strresponse))
#return(textwrap.fill(str(response), 80))
return(f"{response}")
#display(Markdown(f"<b>{response}</b>")
#return "Hello " + query + "!"
#demo = gr.Interface(fn=greet, inputs="text", outputs="text")
#demo.launch(share=True)
from dotenv import load_dotenv
import os
#load_dotenv('config/.env')  # relative to current working directory
# Or, using pathlib for robust path handling
from pathlib import Path
load_dotenv(Path('secrets') / '.env')
api_key = os.getenv("OPENAI_API_KEY")
library(reticulate, quietly = T)
#suppressPackageStartupMessages(library("reticulate"))
options(reticulate.repl.quiet = TRUE)
#use_virtualenv("r-reticulate")
#use_python("/usr/local/bin/python3")
#library(reticulate)
reticulate::py_config()
reticulate::repl_python()
