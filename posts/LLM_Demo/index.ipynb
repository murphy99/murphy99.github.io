{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dbe92e6",
   "metadata": {},
   "source": [
    "---\n",
    "title: Easily Build Customized LLMs\n",
    "date: \"2025-07-15\"\n",
    "image: llm_photo.png\n",
    "categories: [Python, OPENAI, LLMs, Jupyter, OBBBA]\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b5d5a2",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Whether you refer to the One Big Beautiful Bill or the One Big Bad Bill - this LLM demonstrates how to query the bill that is now law. Just how many unexposed nuggets buried in this bill is anyone's guess. In the\n",
    "this case, it's a useful way to query it. It does a pretty good job and you can see how you structure your query makes a difference on the response. Some prompts will be better than others. We listed some prompts below to get you started.\n",
    "\n",
    "One of the use cases for localized LLMs is to help digest new bills, policies, and or court decisions in an efficient and expedient manner. Journalists frequently encounter brand new material before it becomes public and cannot store the document on any server. \n",
    "\n",
    "For best results, we have found that adding other analysis improves the responses and contexts, but here we are just going to have the OBBA summary from the government website as a demonstration on how this can be used and test it to see if it would be comprehensive enough for journalists to use.  \n",
    "\n",
    "It's important to check to make sure the responses are accurate. The 2 source documents are listed below. Including the section number in the query will make verifying the information easier - since you can just look it up. \n",
    "\n",
    "This model uses minimally trained model from OPENAI, and fairly inexpensive of all the models to do the query. Since the subject matter is so narrow - we do not need a big model - just big enough to be fairly responsive to our queries. \n",
    "\n",
    "Adding more PDFs will get a richer and more comprehensive output. Working on your queries or prompts will also improve the results. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61f036d",
   "metadata": {},
   "source": [
    "### Python Packages Required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f802c1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install in packages (pip) in terminal - if missing\n",
    "#!pip install python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "#pip install duckdb\n",
    "import duckdb\n",
    "#pip install llama_index_core\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "import os #built in package\n",
    "#pip install openai\n",
    "import openai\n",
    "#pip install textwrap\n",
    "import textwrap \n",
    "#pip install llama_index.vector_stores.duckdb\n",
    "import llama_index.vector_stores.duckdb\n",
    "#pip install llama-index-embeddings-openai\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "#pip install llama-index-llms-openai\n",
    "from llama_index.llms.openai import OpenAI\n",
    "#pip install gradio\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf14a0a",
   "metadata": {},
   "source": [
    "The LLM creates a vector store everytime it runs. So we delete it before storing the new vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3698a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File deleted successfully\n"
     ]
    }
   ],
   "source": [
    "file_path = 'persist/my_vector_store.duckdb'\n",
    "\n",
    "# Check if file exists\n",
    "if os.path.exists(file_path):\n",
    "  #Delete the file\n",
    "  os.remove(file_path)\n",
    "  print(\"File deleted successfully\")\n",
    "else:\n",
    "  print(\"File doesn't exist - first run - it's all good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ae6bc5",
   "metadata": {},
   "source": [
    "Next we find the OPENAI key and set up the environment so that we are able to use its index capability and using llama indexing provided by Meta's open source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63a6ebe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "#load_dotenv()\n",
    "# Point to the secrets/.env file\n",
    "load_dotenv(dotenv_path=\"secrets/.env\")\n",
    "\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cab274b",
   "metadata": {},
   "source": [
    "Here we import the indexing packages to store the indexing in DuckDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed50eaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.duckdb import DuckDBVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "\n",
    "vector_store = DuckDBVectorStore(\"my_vector_store.duckdb\", persist_dir=\"persist/\")\n",
    "documents = SimpleDirectoryReader(input_dir=\"OBBBA/\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed887062",
   "metadata": {},
   "source": [
    "This is where storage_context points to your indexed PDFs in the storage you specified above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05d50ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2180de",
   "metadata": {},
   "source": [
    "## Deployment \n",
    "\n",
    "This is a basic interface so that you can query anything about the PDF(s) you stored. In this case we stored two documents from https://www.congress.gov/: \n",
    "\n",
    "[https://www.congress.gov/bill/119th-congress/house-bill/1/text](https://www.congress.gov/bill/119th-congress/house-bill/1/text)  \n",
    "\n",
    "[https://www.congress.gov/bill/119th-congress/house-bill/1](https://www.congress.gov/bill/119th-congress/house-bill/1)  \n",
    "\n",
    "We printed them to PDFs and stored them under the OBBBA directory. \n",
    "\n",
    " Some responses will not be what you expect and even provide inaccurate in incomplete responses. This is the nature of LLMs and is good to see it's limitations. The prompts below have been tested and provide pretty accurate information from what I can see. The prompt area is live, so have fun while the funding lasts. \n",
    "\n",
    "**Useful Prompts:**\n",
    "\n",
    "**\"What is the summarization of OBBBA?\"**\n",
    "\n",
    "**\"What are the recissions, modifications and extensions of the programs and or organizations listed in the OBBBA? List  by appearance with section number and title description. Output in list\"**\n",
    "\n",
    "**\"What are the SNAP and Medicaid of the programs and or organizations listed in the OBBBA? List  by appearance with section number and title description. Output in LIst\"**\n",
    "\n",
    "**\"What is the DOJ fund named \"BIDEN.\" What  is it used for? list by item. What provision is it under?\"**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "227d5db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* Running on public URL: https://7b2e0b9c17dd2f97fb.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://7b2e0b9c17dd2f97fb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Create a custom theme with blue as the primary color\n",
    "theme = gr.themes.Default()  \n",
    "#theme = gr.themes.Default(primary_hue=\"blue\", font=[\"Helvetica\", \"sans-serif\"])\n",
    "\n",
    "def greet(query):\n",
    "    query_engine = index.as_query_engine()\n",
    "    response = query_engine.query(query)\n",
    "    return str(response)\n",
    "\n",
    "gr.Interface(\n",
    "  fn=greet,\n",
    "  inputs=gr.Textbox(lines=1, placeholder=\"Enter you query here...\",\n",
    "  label=\"Your Query\"),\n",
    "  outputs=gr.Textbox(label=\"Response\")).launch(share=True, pwa=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
