[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GoData.ca",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nEasily Build Private Small LLMs\n\n\n\n\n\n\nLLMs\n\n\nPython\n\n\nOpenAI\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\nJun 3, 2025\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome\n\n\n\n\n\n\nQuarto\n\n\n\n\n\n\n\n\n\nJun 1, 2025\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/LLM_Demo/index.html",
    "href": "posts/LLM_Demo/index.html",
    "title": "Easily Build Private Small LLMs",
    "section": "",
    "text": "Introduction\nPrivate LLMs can be created to enable you to have full control of your own propiertary PDFs or text files stored locally with just a few lines of code. Journalists who are always on deadliine, can respond quickly to new regulations or court hearing decisions speeding up the time to digest the new material and be the first to respond without risk of exposing their material to the world.\nDeployed on Hugging Face the Private LLM contains documents of publicly available Fables from around the world. With a simple interface, you can query about Aesop or Grimms Fairy tales. Query fables that exists in other parts of the world. Of course this is a demo and the documents are public, but if you clone the LLM demo, then run it locally - you will have your own localized LLM. This LLM is ideal if you want to combine copyrighted material that the larger LLMs may not have been trained on. You may have clients that are risk adverse about having any propietary documents even on a secured server. It’s also great as a demo if you would like to show what LLMs can do, and most importantly what their limitations are.\nSo, lets get started:\nSetting up reticulate\n\nlibrary(reticulate, quietly = T)\n#suppressPackageStartupMessages(library(\"reticulate\"))\noptions(reticulate.repl.quiet = TRUE)\n#use_virtualenv(\"r-reticulate\")\n#use_python(\"/usr/local/bin/python3\")\n\nGoing to try using pyenv go_llm\n\n#library(reticulate)\nreticulate::py_config()\n\npython:         /Users/Eileen/.virtualenvs/godata/bin/python\nlibpython:      /Users/Eileen/.pyenv/versions/3.12.6/lib/libpython3.12.dylib\npythonhome:     /Users/Eileen/.virtualenvs/godata:/Users/Eileen/.virtualenvs/godata\nvirtualenv:     /Users/Eileen/.virtualenvs/godata/bin/activate_this.py\nversion:        3.12.6 (main, Sep 26 2024, 09:29:53) [Clang 15.0.0 (clang-1500.3.9.4)]\nnumpy:          /Users/Eileen/.virtualenvs/godata/lib/python3.12/site-packages/numpy\nnumpy_version:  2.2.2\n\nNOTE: Python version was forced by VIRTUAL_ENV\n\n\nPython packages required\n\n\n# Install core components\n#pip3 install llama-index-core \n#pip3 install python-dotenv \n#pip3 install duckdb \n#pip3 install gradio\n\n# Install integrations\n#pip3 install llama-index-llms-openai\n#pip3 install llama-index-embeddings-openai\n#pip3 install llama_index.vector_stores.duckdb\n\n\n\n# Install integrations\n#pip install llama-index-llms-openai\n#pip install llama-index-embeddings-openai\n#pip install dotenv\n\nimport duckdb\nfrom dotenv import load_dotenv\nimport os\nimport openai\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nimport textwrap\nimport gradio as gr\n\nThis LLM creates a vector store, that we need to make sure is deleted from any previous runs.\n\nfile_path = 'persist/my_vector_store.duckdb'\n\n# Check if file exists\nif os.path.exists(file_path):\n  #Delete the file\n  os.remove(file_path)\n  print(\"File deleted successfully\")\nelse:\n  print(\"File doesn't exist - first run - it's all good\")\n\nFile deleted successfully\n\n\nNext we go get the openai key, if we don’t already have one and set up the environment so that we are able to access the openai indexing capability using llama indexing provided by Meta open source.\n\nfrom dotenv import load_dotenv\nimport os\n\n#load_dotenv('config/.env')  # relative to current working directory\n\n# Or, using pathlib for robust path handling\nfrom pathlib import Path\nload_dotenv(Path('secrets') / '.env')\n\nTrue\n\n\napi_key = os.getenv(\"OPENAI_API_KEY\")\n\nHere we import the indexing packages to store the indexing in DuckDB.\n\n\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.vector_stores.duckdb import DuckDBVectorStore\nfrom llama_index.core import StorageContext\n\nvector_store = DuckDBVectorStore(\"my_vector_store.duckdb\", persist_dir=\"./persist/\")\ndocuments = SimpleDirectoryReader(\"/Users/Eileen/Desktop/GoData/Blog/posts/LLM_Demo/PDFs/\").load_data()\n\nThis is where storage_context points to your indexed PDFs in the storage you specified above\n\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\nindex = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n\nThis is a basic interface so that you can query anything about the PDFs you stored.\n\n# Create a custom theme with blue as the primary color\ntheme = gr.themes.Default()  \n\n\ndef greet(query):\n    \n    query_engine = index.as_query_engine()\n    response = query_engine.query(query)\n    strresponse = str(response)\n    #return(gradio.Markdown(strresponse))\n    #return(textwrap.fill(str(response), 80))\n    return(f\"{response}\")\n    #display(Markdown(f\"&lt;b&gt;{response}&lt;/b&gt;\")\n    #return \"Hello \" + query + \"!\"\n\n#demo = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\n#demo.launch(share=True)   \n\nThis is output, you can now query the public PDFs here on Hugging Face to see how it works. This output is similar but not generated from the code above. There is a seperate script generating the LLM on Hugging Face that you can access by cloning the repository at https://huggingface.co/spaces/GoData/Fables-beta/tree/main.\nThis is from: https://huggingface.co/spaces/GoData/Fables-beta"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Eileen P. Murphy",
    "section": "",
    "text": "This blog centers on the exploration of data discovery and practical data science for implementing the use of machine learning, dashboards, LLMs, LLM Apps by using existing open source tools such as but not limited to R, Python, DuckDB, SQLite, MongoDB, Hadoop, and Spark.\nGoData.ca mission is to explore and replicate already available research and implement actual use cases for implementation.\nEileen Murphy is the owner and contributor to GoData.ca blog."
  },
  {
    "objectID": "about.html#about-the-blog",
    "href": "about.html#about-the-blog",
    "title": "Eileen P. Murphy",
    "section": "",
    "text": "This blog centers on the exploration of data discovery and practical data science for implementing the use of machine learning, dashboards, LLMs, LLM Apps by using existing open source tools such as but not limited to R, Python, DuckDB, SQLite, MongoDB, Hadoop, and Spark.\nGoData.ca mission is to explore and replicate already available research and implement actual use cases for implementation.\nEileen Murphy is the owner and contributor to GoData.ca blog."
  },
  {
    "objectID": "posts/Welcome/index.html",
    "href": "posts/Welcome/index.html",
    "title": "Welcome",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "posts/Welcome/index.html#quarto",
    "href": "posts/Welcome/index.html#quarto",
    "title": "Welcome",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "posts/Welcome/index.html#running-code",
    "href": "posts/Welcome/index.html#running-code",
    "title": "Welcome",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "walkthrough.html",
    "href": "walkthrough.html",
    "title": "Hello, Quarto",
    "section": "",
    "text": "Markdown is an easy to read and write text format:\n\nIt’s plain text so works well with version control\nIt can be rendered into HTML, PDF, and more\nLearn more at: https://quarto.org/docs/authoring/"
  },
  {
    "objectID": "walkthrough.html#markdown",
    "href": "walkthrough.html#markdown",
    "title": "Hello, Quarto",
    "section": "",
    "text": "Markdown is an easy to read and write text format:\n\nIt’s plain text so works well with version control\nIt can be rendered into HTML, PDF, and more\nLearn more at: https://quarto.org/docs/authoring/"
  },
  {
    "objectID": "walkthrough.html#code-cell",
    "href": "walkthrough.html#code-cell",
    "title": "Hello, Quarto",
    "section": "Code Cell",
    "text": "Code Cell\nHere is a Python code cell:\n\nimport os\nos.cpu_count()\n\n8"
  },
  {
    "objectID": "walkthrough.html#equation",
    "href": "walkthrough.html#equation",
    "title": "Hello, Quarto",
    "section": "Equation",
    "text": "Equation\nUse LaTeX to write equations:\n\\[\n\\chi' = \\sum_{i=1}^n k_i s_i^2\n\\]"
  }
]