{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dbe92e6",
   "metadata": {},
   "source": [
    "---\n",
    "title: Easily Build Customized LLMs\n",
    "date: \"2025-07-15\"\n",
    "image: llm_photo.png\n",
    "categories: [Python, OPENAI, LLMs, Jupyter, OBBBA]\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "One of the use cases for localized LLMs is to help digest new bills, policies, and or court decisions in an efficient and expedient manner. Journalists frequently encounter brand new material before it becomes public and cannot store the document on any server.\n",
    "\n",
    "In this model, you need to add a complex, detailed, and involved document, in this case it’s the summary of the “One Big Beautiful Bill Act” or some might call the “One Big Bad Bill Act.” Whatever you call it, we’ll refer to it as the OBBBA. It does a pretty good job and you can see how you structure your query makes a difference on the response. Some prompts will be better than others.\n",
    "\n",
    "For best results, I have found that adding other analysis improves the responses and contexts, but here we are just going to have the OBBA summary and text from the government website as a demonstration on how this can be used and test it to see if it would be comprehensive enough for journalists to use.\n",
    "\n",
    "This model uses minimally trained model from OPENAI, and fairly inexpensive of all the models to do the query. Since the subject matter is so narrow - we do not need a big model - just big enough to be fairly responsive to our queries.\n",
    "\n",
    "Adding more PDFs will get a richer and more comprehensive output. Working on your queries or prompts will also improve the results.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61f036d",
   "metadata": {},
   "source": [
    "### Python Packages Required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f802c1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install in packages (pip) in terminal - if missing\n",
    "#!pip install python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "#pip install duckdb\n",
    "import duckdb\n",
    "#pip install llama_index_core\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "import os #built in package\n",
    "#pip install openai\n",
    "import openai\n",
    "#pip install textwrap\n",
    "import textwrap \n",
    "#pip install llama_index.vector_stores.duckdb\n",
    "import llama_index.vector_stores.duckdb\n",
    "#pip install llama-index-embeddings-openai\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "#pip install llama-index-llms-openai\n",
    "from llama_index.llms.openai import OpenAI\n",
    "#pip install gradio\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf14a0a",
   "metadata": {},
   "source": [
    "The LLM creates a vector store everytime it runs. So we delete it before storing the new vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3698a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File deleted successfully\n"
     ]
    }
   ],
   "source": [
    "file_path = 'persist/my_vector_store.duckdb'\n",
    "\n",
    "# Check if file exists\n",
    "if os.path.exists(file_path):\n",
    "  #Delete the file\n",
    "  os.remove(file_path)\n",
    "  print(\"File deleted successfully\")\n",
    "else:\n",
    "  print(\"File doesn't exist - first run - it's all good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ae6bc5",
   "metadata": {},
   "source": [
    "Next we find the OPENAI key and set up the environment so that we are able to use its index capability and using llama indexing provided by Meta's open source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63a6ebe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "#load_dotenv()\n",
    "\n",
    "load_dotenv(dotenv_path=\"secrets/.env\")\n",
    "\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cab274b",
   "metadata": {},
   "source": [
    "Here we import the indexing packages to store the indexing in DuckDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed50eaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.duckdb import DuckDBVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "vector_store = DuckDBVectorStore(\"my_vector_store.duckdb\", persist_dir=\"persist/\")\n",
    "documents = SimpleDirectoryReader(\"/Users/Eileen/Desktop/GoData/Blog/posts/LLM_Demo/OBBBA/\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed887062",
   "metadata": {},
   "source": [
    "This is where storage_context points to your indexed PDFs in the storage you specified above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05d50ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2180de",
   "metadata": {},
   "source": [
    "## Deployment \n",
    "\n",
    "This is a basic interface so that you can query anything about the PDF(s) you stored. In this case we stored two documents from [https://www.congress.gov/](https://www.congress.gov/):\n",
    "\n",
    "[https://www.congress.gov/bill/119th-congress/house-bill/1/text](https://www.congress.gov/bill/119th-congress/house-bill/1/text)\n",
    "\n",
    "[https://www.congress.gov/bill/119th-congress/house-bill/1](https://www.congress.gov/bill/119th-congress/house-bill/1])\n",
    "\n",
    "We printed them to PDFs and stored them under the OBBBA directory.\n",
    "\n",
    "Some responses will not be what you expect and even provide inaccurate in incomplete responses. The reponses have limited output, so it's best to narrow your questions as much as possible and to provide the sections and provisions for verifications.  The prompts below have been tested and can be verified with the above documents. The prompt area is live, so have fun and dig in.\n",
    "\n",
    "\n",
    "This is a basic interface so that you can query anything about the PDF(s) stored.\n",
    "\n",
    "**Suggested prompts for potential stories:**\n",
    "\n",
    "What is OBBBA?\n",
    "\n",
    "How are Medicaid programs affected? List provision or section number for verification. Output in list.\n",
    "\n",
    "How are SNAP programs affected? List provision or section number for verification. Output list. \n",
    "\n",
    "What are all the environmental reductions, rescinds, and rescissions? List provisions and section number for verification. Output to list.\n",
    "\n",
    "What are all the environmental increases, and or establishment appropriations (not rescissions)? List provisions and section number for verification. Output to list.\n",
    "\n",
    "What are all the welfare reductions, and rescissions. List provisions and section number for verification. Output to list.\n",
    "\n",
    "What are all the law enforcement reductions, and rescissions. List provisions and section number for verification. Output to list.\n",
    "\n",
    "What are all the  law enforcement increases and or appropriations? List provisions and section number for verification. Output to list.\n",
    "\n",
    "What are all the welfare appropriations, and increases?. List provisions and section number for verification. Output to list.\n",
    "\n",
    "What is the DOJ fund named ”BIDEN.” What is it used for? list by item. What provision is it under? How  much is being appropriated for it?\n",
    "\n",
    "How is FEMA being appropriated? List the provision or section number  along with description for verification. Is FEMA still going to be used for disaster recovery?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "227d5db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def greet(query):\n",
    "    query_engine = index.as_query_engine()\n",
    "    response = query_engine.query(query)\n",
    "    return str(response)\n",
    "\n",
    "gr.Interface(\n",
    "  fn=greet,\n",
    "  inputs=gr.Textbox(lines=1, placeholder=\"Enter your query here...\",\n",
    "  label=\"Your Query\"),\n",
    "  outputs=gr.Textbox(label=\"Response\")\n",
    "  ).launch(share=False, pwa=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98f57d2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
