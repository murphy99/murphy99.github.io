---
title: "Easily Build Customized LLMs"
image: llm_fables.jpg
date: "2025-06-03"
embed-resources: true
categories: [LLMs, Python, OpenAI, Hugging Face]
#jupyter: python3
#filters:
 # - gradio
---

# Introduction

Customized LLMs can be created to enable you to have full control of your own propiertary PDFs or text files stored locally with just a few lines of code. Journalists who are always on deadliine, can respond quickly to new regulations or court hearing decisions speeding up the time to digest the new material and be the first to respond without risk of exposing their material to the world.

Deployed on Hugging Face the Private LLM contains documents of publicly available Fables from around the world. With a simple interface, you can query about Aesop or Grimms Fairy tales. Query fables that exists in other parts of the world. Of course this is a demo and the documents are public, but if you clone the LLM demo, then run it locally - you will have your own localized LLM. This LLM is ideal if you want to combine copyrighted material that the larger LLMs may not have been trained on. You may have clients that are risk adverse about having any propietary documents even on a secured server. It's also great as a demo if you would like to show what LLMs can do, and most importantly what their limitations are.

So, lets get started:

Setting up reticulate

```{r}
library(reticulate, quietly = T)
#suppressPackageStartupMessages(library("reticulate"))
options(reticulate.repl.quiet = TRUE)
#use_virtualenv("r-reticulate")
#use_python("/usr/local/bin/python3")
```

Going to try using pyenv go_llm

```{r}
#library(reticulate)
reticulate::py_config()
```

Python packages required

```{bash}

# Install core components
#pip3 install llama-index-core 
#pip3 install python-dotenv 
#pip3 install duckdb 
#pip3 install gradio

# Install integrations
#pip3 install llama-index-llms-openai
#pip3 install llama-index-embeddings-openai
#pip3 install llama_index.vector_stores.duckdb
```

```{python}

# Install integrations
#pip install llama-index-llms-openai
#pip install llama-index-embeddings-openai
#pip install dotenv

import duckdb
from dotenv import load_dotenv
import os
import openai
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
import textwrap
import gradio as gr

```

This LLM creates a vector store, that we need to make sure is deleted from any previous runs.

```{python}

file_path = 'persist/my_vector_store.duckdb'

# Check if file exists
if os.path.exists(file_path):
  #Delete the file
  os.remove(file_path)
  print("File deleted successfully")
else:
  print("File doesn't exist - first run - it's all good")

```

Next we go get the openai key, if we don't already have one and set up the environment so that we are able to access the openai indexing capability using llama indexing provided by Meta open source.

```{python}

from dotenv import load_dotenv
import os

#load_dotenv('config/.env')  # relative to current working directory

# Or, using pathlib for robust path handling
from pathlib import Path
load_dotenv(Path('secrets') / '.env')

api_key = os.getenv("OPENAI_API_KEY")


```

Here we import the indexing packages to store the indexing in DuckDB.

```{python}

from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.vector_stores.duckdb import DuckDBVectorStore
from llama_index.core import StorageContext

vector_store = DuckDBVectorStore("my_vector_store.duckdb", persist_dir="./persist/")
documents = SimpleDirectoryReader("/Users/Eileen/Desktop/GoData/Blog/posts/LLM_Demo/PDFs/").load_data()

```

This is where storage_context points to your indexed PDFs in the storage you specified above

```{python}
storage_context = StorageContext.from_defaults(vector_store=vector_store)
index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)
```

This is a basic interface so that you can query anything about the PDFs you stored.

```{python}
#| label: my-chunk
#| eval: false
#| include: false

import gradio as gr

def greet(query):
    print("Before query")  # Debugging
    query_engine = index.as_query_engine()
    response = query_engine.query(query)
    print("After query")   # Debugging
    return str(response)


gr.Interface(fn=greet, 
             inputs=gr.Textbox(lines=1, placeholder="Enter your query here..."), 
             outputs=gr.Textbox()).launch()

```

```{python}
# Create a custom theme with blue as the primary color
theme = gr.themes.Default()  


def greet(query):
    
    query_engine = index.as_query_engine()
    response = query_engine.query(query)
    strresponse = str(response)
    #return(gradio.Markdown(strresponse))
    #return(textwrap.fill(str(response), 80))
    return(f"{response}")
    #display(Markdown(f"<b>{response}</b>")
    #return "Hello " + query + "!"

#demo = gr.Interface(fn=greet, inputs="text", outputs="text")
#demo.launch(share=True)   
```

This is output, you can now query the public PDFs here on Hugging Face to see how it works. This output is similar but not generated from the code above. There is a seperate script generating the LLM on Hugging Face that you can access by cloning the repository at https://huggingface.co/spaces/GoData/Fables-beta/tree/main.

This is from: https://huggingface.co/spaces/GoData/Fables-beta

```{=html}
<iframe width="780" height="500" src="https://godata-fables-beta.hf.space/" title="Quarto Documentation"></iframe>
```
