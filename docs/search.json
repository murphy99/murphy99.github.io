[
  {
    "objectID": "posts/Shinylive/index.html",
    "href": "posts/Shinylive/index.html",
    "title": "Converting Shiny Apps to ShinyLive",
    "section": "",
    "text": "Shiny Apps - can be converted to Shinylive using Quarto in just a few lines of code. If the Shiny app isn’t too complex, we can provide a serverless interactive experience using just an html website. The Shiny App that we converted is one of Posit’s simple Shiny Apps in its gallery (Posit 2014). ShinyLive allows a more frictionless way to share your app without having to use a hosted server by running on its own html page served to the client. The html is rendered in quarto to automatically download the needed assets to the client and it can run from their own pc or mac.\nThe Shiny App that we chose is the KMeans model using the Iris dataset, provides a way to convert a Shiny App, but it’s also a handy tool to deploy on our website. There were just 2 things we had to change and that was using the {shinylive-r} instead of {r} and to change ui to using fluidPage from the bslib library to ensure app resizes well to different devices. You’ll notice that older shiny apps don’t resize well, and this is why."
  },
  {
    "objectID": "posts/Shinylive/index.html#introduction",
    "href": "posts/Shinylive/index.html#introduction",
    "title": "Converting Shiny Apps to ShinyLive",
    "section": "",
    "text": "Shiny Apps - can be converted to Shinylive using Quarto in just a few lines of code. If the Shiny app isn’t too complex, we can provide a serverless interactive experience using just an html website. The Shiny App that we converted is one of Posit’s simple Shiny Apps in its gallery (Posit 2014). ShinyLive allows a more frictionless way to share your app without having to use a hosted server by running on its own html page served to the client. The html is rendered in quarto to automatically download the needed assets to the client and it can run from their own pc or mac.\nThe Shiny App that we chose is the KMeans model using the Iris dataset, provides a way to convert a Shiny App, but it’s also a handy tool to deploy on our website. There were just 2 things we had to change and that was using the {shinylive-r} instead of {r} and to change ui to using fluidPage from the bslib library to ensure app resizes well to different devices. You’ll notice that older shiny apps don’t resize well, and this is why."
  },
  {
    "objectID": "posts/Shinylive/index.html#kmeans",
    "href": "posts/Shinylive/index.html#kmeans",
    "title": "Converting Shiny Apps to ShinyLive",
    "section": "KMeans",
    "text": "KMeans\nKMeans is an unsupervised machine learning algorithm used to group similar data together without already predefined classifications. All the points are clustered around a centroid that represents the mean of all points within the cluster. The number of centroids are referred to as “K values” and represents the number groups within all the data ponts. The goal is to minimize the euclidean distance between the data point and the its centroid mean, maximize the separation between clusters, and minimize total squared distance between each point and it’s assigned centriod.\nIn order to find the optimal K values, you need to run through and iterative process. This KMeans model uses Shinylive to be able to iterate quickly between K values to visually see the best grouping or at least to narrow down the K values to calculate the optimal number of categories.\n\n#install.packages(\"shiny\")\n#install.packages(\"shinylive\")\nlibrary(shiny)\nlibrary(shinylive)\nlibrary(bslib)\n\n\nAttaching package: 'bslib'\n\n\nThe following object is masked from 'package:utils':\n\n    page\n\n\n\nUpdate Quarto Extension\nRun this in terminal: quarto add quarto-ext/shinylive\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 500\n\n\nvars &lt;- setdiff(names(iris), \"Species\")\n\nui &lt;- fluidPage(\n  titlePanel('Iris k-means clustering'),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput('xcol', 'X Variable', vars),\n      selectInput('ycol', 'Y Variable', vars, selected = vars[[2]]),\n      numericInput('clusters', 'Cluster count', 3, min = 1, max = 9)\n    ),\n    mainPanel(\n      plotOutput('plot1')\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  selectedData &lt;- reactive({\n    iris[, c(input$xcol, input$ycol)]\n  })\n  clusters &lt;- reactive({\n    kmeans(selectedData(), input$clusters)\n  })\n  output$plot1 &lt;- renderPlot({\n    palette(c(\"#E41A1C\", \"#377EB8\", \"#4DAF4A\", \"#984EA3\",\n              \"#FF7F00\", \"#FFFF33\", \"#A65628\", \"#F781BF\", \"#999999\"))\n    par(mar = c(5.1, 4.1, 0, 1))\n    plot(selectedData(),\n         col = clusters()$cluster,\n         pch = 20, cex = 3)\n    points(clusters()$centers, pch = 4, cex = 4, lwd = 4)\n  })\n}\n\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "posts/Shinylive/index.html#deployment",
    "href": "posts/Shinylive/index.html#deployment",
    "title": "Converting Shiny Apps to ShinyLive",
    "section": "Deployment",
    "text": "Deployment\nWe can take a shiny app and convert it to shiny live that enables the application to run on just the client’s browser from an html page.\n\nVerify you can retrieve the shinylive assets\n\n#shinylive::assets_info()\n\n\n\nYAML (Remove comments and add to your Yaml) :\n\n# ---  \n# format:   \n#   html:  \n#     embed-resources: false  \n#   filters:  \n#     - shinylive  \n# ---"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GoData.ca",
    "section": "",
    "text": "Data Pipelines with Pyspark\n\n\n\nPyspark\n\nPython\n\nR\n\nCAFC\n\nDownloads\n\n\n\n\n\n\n\n\n\nAug 15, 2025\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\nEasily Build Customized LLMs\n\n\n\nPython\n\nOPENAI\n\nLLMs\n\nJupyter\n\nOBBBA\n\n\n\n\n\n\n\n\n\nJul 15, 2025\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\nConverting Shiny Apps to ShinyLive\n\n\n\nR\n\nShinylive\n\nKMeans\n\n\n\n\n\n\n\n\n\nJun 30, 2025\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\nRSS Feeds\n\n\n\nR\n\nJavascript\n\ntidyRSS\n\n\n\n\n\n\n\n\n\nJun 15, 2025\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\nWelcome\n\n\n\nQuarto\n\n\n\n\n\n\n\n\n\nJun 2, 2025\n\n2 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/RSS_Reader/index0.html",
    "href": "posts/RSS_Reader/index0.html",
    "title": "RSS Feeds",
    "section": "",
    "text": "RSS Readers may not be in fashion but I believe they are on the comeback. Inspired by an article from InfoWorld(Machlis 2022), with some slight modifications, I created an rss feed to list many the research publications. The list of research papers can look overwhelming, but can be refined or filtered using the search boxes.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\nlibrary(DT)\nlibrary(purrr)\nlibrary(stringr)\nlibrary(lubridate)\nlibrary(tidyRSS)\n\n\nflag=0\n\nmy_feed &lt;- \"https://rss.arxiv.org/rss/cs.LG\"\n\nresult &lt;- tryCatch(\n  {\n    my_feed_data &lt;- tidyfeed(my_feed)\n    cat(\"Number of items:\", nrow(my_feed_data), \"\\n\")\n    my_feed_data |&gt; \n      select(feed_pub_date, item_title, item_link, item_description)\n  },\n  error = function(e) {\n    cat(\"Error caught:\", e$message, \"\\n\")\n    NULL  # return NULL on error\n  }\n)\n\nGET request successful. Parsing...\n\n\nError caught: subscript out of bounds \n\nif (is.null(result) || nrow(result) == 0) {\n  flag=1\n  cat(\"No papers published today\\n\")\n} else {\n  print(result)\n}\n\nNo papers published today\n\n\n\nif (! flag) {\n  my_feed_data_summary &lt;- my_feed_data |&gt;\n    select(item_title, feed_pub_date, item_link,item_description) }\n\n\nif (! flag) {\n#changed item_title to item_desc\nmy_rss_feed &lt;- my_feed_data_summary |&gt; mutate(\n    item_title = str_glue(\"&lt;a target='_blank' title='{item_title}' href='{item_link}' rel='noopener'&gt;{item_title}&lt;/a&gt;\")\n)}\n\n\nif (! flag) {\n  my_rss_feed_table &lt;- my_rss_feed |&gt; select(-item_link)}\n#my_feed_data_summary\n\n\n\nif (! flag) {\n  DT::datatable(my_rss_feed_table, filter = 'top', escape = FALSE, rownames = FALSE,\n    options = list(\n    search = list(regex = TRUE, caseInsensitive = TRUE),  \n    pageLength = 10,\n    lengthMenu = c(10, 25, 50, 100, 200),\n    autowidth = TRUE,\n   columnDefs = list(list(width = '80%', targets = list(2)))\n    )\n  )}\n\n\n\n\n\n\n\n\n\nReferences\n\nMachlis, Sharon. 2022. “How to Create Your Own RSS Reader with r _ InfoWorld.” InfoWorld, December. https://www.infoworld.com/article/2337738/how-to-create-your-own-rss-reader-with-r.html."
  },
  {
    "objectID": "posts/RSS_Reader/Most_Recent_Papers.html",
    "href": "posts/RSS_Reader/Most_Recent_Papers.html",
    "title": "Recent arXiv Research Papers (Updated M-F)",
    "section": "",
    "text": "GET request successful. Parsing...\n\n\nRecords Retrieved: 196 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/RSS_Reader/index.html",
    "href": "posts/RSS_Reader/index.html",
    "title": "RSS Feeds",
    "section": "",
    "text": "There are more research papers being published than we could ever consume. This RSS Reader gathers the arXiv papers published today and allows us to search through by keywords.\nInspired by an article from InfoWorld (Machlis 2022). Future feads will include more research papers from platforms into a database allowing for a more thorough historical search.\n\n\n\n# setup includes libraries and functions\n\nsource(\"RSS_functions.R\",chdir=T)\n\n\nContent of RSS_functions.R script\n\n# get path to script\npathway &lt;- here::here(\"posts\",\"RSS_Reader\", \"RSS_functions.R\")\n\n# generate output of lines from script\n\nlines &lt;- readLines(pathway, warn=FALSE)\ncat(lines, sep = \"\\n\")\n\n# Get the most recent papers function\n\n# Required Libraries\n\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(dplyr))\nlibrary(DT)\nlibrary(tidyRSS)\n\nmost_recent_test &lt;- function(source) {\n  \n  site=source\n  \n  my_feed_data &lt;- tidyfeed(site) |&gt;\n    select(feed_pub_date,item_title, item_link, item_description)\n  \n  my_feed_data_summary &lt;- my_feed_data |&gt;\n    select(item_title, feed_pub_date, item_link,\n           item_description) \n  \n  #changed item_title to item_desc\n  my_rss_feed &lt;- my_feed_data_summary |&gt; mutate(\n    item_title = str_glue(\"&lt;a target='_blank' title='{item_title}' href='{item_link}' rel='noopener'&gt;{item_title}&lt;/a&gt;\")\n  )\n  \n  my_rss_feed_table &lt;- my_rss_feed |&gt; select(-item_link)\n  #my_feed_data_summary\n  \n  \n  return(my_rss_feed_table)  \n}\n\nmost_recent &lt;- function(source) {\n  tryCatch({\n    site &lt;- source\n    my_feed_data &lt;- tidyfeed(site) |&gt;\n      select(feed_pub_date, item_title, item_link, item_description)\n    \n    my_feed_data_summary &lt;- my_feed_data |&gt;\n      select(item_title, feed_pub_date, item_link, item_description) \n    \n    my_rss_feed &lt;- my_feed_data_summary |&gt; mutate(\n      item_title = str_glue(\"&lt;a target='_blank' title='{item_title}' href='{item_link}' rel='noopener'&gt;{item_title}&lt;/a&gt;\")\n    )\n    \n    # Return only the most recent record\n    my_rss_feed_table &lt;- my_rss_feed |&gt; \n      arrange(desc(feed_pub_date)) |&gt; \n      #slice(1) |&gt;\n      select(-item_link)\n    \n    return(my_rss_feed_table)\n  }, error = function(e) {\n    message(\"Error fetching or parsing feed: \", e$message)\n    #return(NA)\n  })\n}\n\n\n#my_rss_feed_table\n\n\nGetting feed for arXiv and checking the number of results. There are no papers on weekends\n\nflag=0 #Set Error flag to False\n\n# Call functins in RSS_functions.R\n\nmy_feed &lt;- \"https://rss.arxiv.org/rss/cs.LG\"\n\nrecent_records&lt;-most_recent(my_feed)\n\nGET request successful. Parsing...\n\n#paste(\"recent records: \", recent_records)\n\n\n#print (result_output)\n\n# Sets flag = 1 when no records retrieved\n\nif (is.null(recent_records) || (length(recent_records) == 1 && is.na(recent_records))) {\n  flag &lt;- 1\n  cat(\"Sorry, no papers published today\\n\")\n} else {\n  cat(\"Records Retrieved:\", nrow(recent_records), \"\\n\")\n  flag &lt;- 0\n}\n\nRecords Retrieved: 196 \n\ncat(\"flag:\", flag, \"\\n\")\n\nflag: 0 \n\n\n\n\nCreating output using DT library\n\nif (!flag) {\n  DT::datatable(recent_records, filter = 'top', escape = FALSE, rownames = FALSE,\n    options = list(\n    search = list(regex = TRUE, caseInsensitive = TRUE),  \n    pageLength = 10,\n    lengthMenu = c(10, 25, 50, 100, 200),\n    autowidth = TRUE,\n   columnDefs = list(list(width = '80%', targets = list(2)))\n    )\n  )}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\nReferences\n\nMachlis, Sharon. 2022. “How to Create Your Own RSS Reader with r _ InfoWorld.” InfoWorld, December. https://www.infoworld.com/article/2337738/how-to-create-your-own-rss-reader-with-r.html."
  },
  {
    "objectID": "walkthrough.html",
    "href": "walkthrough.html",
    "title": "Hello, Quarto",
    "section": "",
    "text": "Markdown is an easy to read and write text format:\n\nIt’s plain text so works well with version control\nIt can be rendered into HTML, PDF, and more\nLearn more at: https://quarto.org/docs/authoring/"
  },
  {
    "objectID": "walkthrough.html#markdown",
    "href": "walkthrough.html#markdown",
    "title": "Hello, Quarto",
    "section": "",
    "text": "Markdown is an easy to read and write text format:\n\nIt’s plain text so works well with version control\nIt can be rendered into HTML, PDF, and more\nLearn more at: https://quarto.org/docs/authoring/"
  },
  {
    "objectID": "walkthrough.html#code-cell",
    "href": "walkthrough.html#code-cell",
    "title": "Hello, Quarto",
    "section": "Code Cell",
    "text": "Code Cell\nHere is a Python code cell:\n\nimport os\nos.cpu_count()\n\n8"
  },
  {
    "objectID": "walkthrough.html#equation",
    "href": "walkthrough.html#equation",
    "title": "Hello, Quarto",
    "section": "Equation",
    "text": "Equation\nUse LaTeX to write equations:\n\\[\n\\chi' = \\sum_{i=1}^n k_i s_i^2\n\\]"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Eileen P. Murphy",
    "section": "",
    "text": "This blog centers on the exploration of data discovery and practical data science for implementing the use of machine learning, dashboards, LLMs, LLM Apps by using existing open source tools such as but not limited to R, Python, DuckDB, SQLite, MongoDB, Hadoop, and Spark.\nGoData.ca mission is to explore and replicate already available research and implement actual use cases for implementation.\nEileen Murphy is the owner and contributor to GoData.ca blog."
  },
  {
    "objectID": "about.html#about-the-blog",
    "href": "about.html#about-the-blog",
    "title": "Eileen P. Murphy",
    "section": "",
    "text": "This blog centers on the exploration of data discovery and practical data science for implementing the use of machine learning, dashboards, LLMs, LLM Apps by using existing open source tools such as but not limited to R, Python, DuckDB, SQLite, MongoDB, Hadoop, and Spark.\nGoData.ca mission is to explore and replicate already available research and implement actual use cases for implementation.\nEileen Murphy is the owner and contributor to GoData.ca blog."
  },
  {
    "objectID": "posts/LLM_Demo/index.html",
    "href": "posts/LLM_Demo/index.html",
    "title": "Easily Build Customized LLMs",
    "section": "",
    "text": "One of the use cases for localized LLMs is to help digest new bills, policies, and/or court decisions in an efficient and expedient manner. Journalists frequently encounter brand new material before it becomes public and cannot store the document on any server.\nIn this model, you need to add a complex, detailed, and involved document, in this case it’s the summary of the “One Big Beautiful Bill Act” or some might call the “One Big Bad Bill Act.” Whatever you call it, we’ll refer to it as the OBBBA. It does a pretty good job and you can see how you structure your query makes a difference on the response. Some prompts will be better than others.\nFor best results, I have found that adding other analysis improves the responses and contexts, but here we are just going to have the OBBA summary and text from the government website as a demonstration on how this can be used and test it to see if it would be comprehensive enough for journalists to use.\nThis model uses minimally trained model from OPENAI, and fairly inexpensive of all the models to do the query. Since the subject matter is so narrow - we do not need a big model - just big enough to be fairly responsive to our queries.\nAdding more PDFs will get a richer and more comprehensive output. Working on your queries or prompts will also improve the results.\n\n\n\n#Install in packages (pip) in terminal - if missing\n#!pip install python-dotenv\nfrom dotenv import load_dotenv\n#pip install duckdb\nimport duckdb\n#pip install llama_index_core\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nimport os #built in package\n#pip install openai\nimport openai\n#pip install textwrap\nimport textwrap \n#pip install llama_index.vector_stores.duckdb\nimport llama_index.vector_stores.duckdb\n#pip install llama-index-embeddings-openai\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n#pip install llama-index-llms-openai\nfrom llama_index.llms.openai import OpenAI\n#pip install gradio\nimport gradio as gr\n\nThe LLM creates a vector store everytime it runs. So we delete it before storing the new vector\n\nfile_path = 'persist/my_vector_store.duckdb'\n\n# Check if file exists\nif os.path.exists(file_path):\n  #Delete the file\n  os.remove(file_path)\n  print(\"File deleted successfully\")\nelse:\n  print(\"File doesn't exist - first run - it's all good\")\n\nFile deleted successfully\n\n\nNext we find the OPENAI key and set up the environment so that we are able to use its index capability and using llama indexing provided by Meta’s open source\n\nfrom dotenv import load_dotenv\n#load_dotenv()\n\nload_dotenv(dotenv_path=\"secrets/.env\")\n\napi_key = os.getenv('OPENAI_API_KEY')\n\nfrom openai import OpenAI\nclient = OpenAI(api_key=api_key)\n\nHere we import the indexing packages to store the indexing in DuckDB.\n\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.vector_stores.duckdb import DuckDBVectorStore\nfrom llama_index.core import StorageContext\n\nvector_store = DuckDBVectorStore(\"my_vector_store.duckdb\", persist_dir=\"persist/\")\ndocuments = SimpleDirectoryReader(\"/Users/Eileen/Desktop/GoData/Blog/posts/LLM_Demo/OBBBA/\").load_data()\n\nThis is where storage_context points to your indexed PDFs in the storage you specified above\n\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\nindex = VectorStoreIndex.from_documents(documents, storage_context=storage_context)"
  },
  {
    "objectID": "posts/Welcome/index.html",
    "href": "posts/Welcome/index.html",
    "title": "Welcome",
    "section": "",
    "text": "This blog is using Quarto. The YAML and the structure of the directories were puzzling to me at first. The directory for each blog entry, YAML configuration files for global and local document settings. The seperation between source and rendered files is a game changer because you can have a zippity site rendered in just html and javascript and another layer of source files used to render and maintain the site. Once you get the hang of it, it’s fun to work on.\n\nQuarto, a publishing pipeline\nAs I was warming up to Quarto - I stumbled on was a book on how to self publish on Amazon using Quarto to generate from text to Kindle format. Software to replace Word? Now, my interest was piqued.\nBut, while impressive isn’t perfect quite yet. While Quarto has a Jupyter python kernal for your notebooks, some of the outputs such as Gridio sends output to the console instead of the terminal. While this is a flaw, I believe the fix - if it hasn’t been solved will be soon.\nIf you are Jupyter notebook and want them working with Quarto right away, Jeremy Howard has an excellent site to get you going with having the best both worlds of integrating Jupyter notebooks and the publishing bells and whistles used in Quarto.\nI could not ignore Quarto, any longer.\nhttps://www.fast.ai/posts/2022-08-25-jupyter-git.html https://www.fast.ai/posts/2022-07-28-nbdev2.html#our-new-secret-weapon-for-productivity\nhttps://nbdev.fast.ai/getting_started.html\nthink python"
  },
  {
    "objectID": "posts/RSS_Reader/Most_Recent_Papers.html#make-sure-that-are-papers-published-today",
    "href": "posts/RSS_Reader/Most_Recent_Papers.html#make-sure-that-are-papers-published-today",
    "title": "Recent arXiv Research Papers (Updated M-F)",
    "section": "",
    "text": "flag=0\n\nmy_feed &lt;- \"https://rss.arxiv.org/rss/cs.LG\"\n\nresult &lt;- tryCatch(\n  {\n    my_feed_data &lt;- tidyfeed(my_feed)\n    cat(\"Number of items:\", nrow(my_feed_data), \"\\n\")\n    my_feed_data |&gt; \n      select(feed_pub_date, item_title, item_link, item_description)\n  },\n  error = function(e) {\n    cat(\"Error caught:\", e$message, \"\\n\")\n    NULL  # return NULL on errorde\n  }\n)\n\nError caught: could not find function \"tidyfeed\" \n\nif (is.null(result) || nrow(result) == 0) {\n  flag=1\n  cat(\"Sorry, no papers published today\\n\")\n} else {\n  print(result)\n}\n\nSorry, no papers published today\n\n\n```"
  },
  {
    "objectID": "posts/RSS_Reader/Most_Recent_Papers.html#define-most-recent-function",
    "href": "posts/RSS_Reader/Most_Recent_Papers.html#define-most-recent-function",
    "title": "Recent arXiv Research Papers (Updated M-F)",
    "section": "Define most recent function",
    "text": "Define most recent function"
  },
  {
    "objectID": "posts/RSS_Reader/Most_Recent_Papers.html#make-sure-that-are-papers-in-the-pipeline-today",
    "href": "posts/RSS_Reader/Most_Recent_Papers.html#make-sure-that-are-papers-in-the-pipeline-today",
    "title": "Recent arXiv Research Papers (Updated M-F)",
    "section": "",
    "text": "GET request successful. Parsing...\n\n\nNumber of items: 179"
  },
  {
    "objectID": "posts/LLM_Demo/index.html#introduction",
    "href": "posts/LLM_Demo/index.html#introduction",
    "title": "Easily Build Customized LLMs",
    "section": "",
    "text": "One of the use cases for localized LLMs is to help digest new bills, policies, and/or court decisions in an efficient and expedient manner. Journalists frequently encounter brand new material before it becomes public and cannot store the document on any server.\nIn this model, you need to add a complex, detailed, and involved document, in this case it’s the summary of the “One Big Beautiful Bill Act” or some might call the “One Big Bad Bill Act.” Whatever you call it, we’ll refer to it as the OBBBA. It does a pretty good job and you can see how you structure your query makes a difference on the response. Some prompts will be better than others.\nFor best results, I have found that adding other analysis improves the responses and contexts, but here we are just going to have the OBBA summary and text from the government website as a demonstration on how this can be used and test it to see if it would be comprehensive enough for journalists to use.\nThis model uses minimally trained model from OPENAI, and fairly inexpensive of all the models to do the query. Since the subject matter is so narrow - we do not need a big model - just big enough to be fairly responsive to our queries.\nAdding more PDFs will get a richer and more comprehensive output. Working on your queries or prompts will also improve the results.\n\n\n\n#Install in packages (pip) in terminal - if missing\n#!pip install python-dotenv\nfrom dotenv import load_dotenv\n#pip install duckdb\nimport duckdb\n#pip install llama_index_core\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nimport os #built in package\n#pip install openai\nimport openai\n#pip install textwrap\nimport textwrap \n#pip install llama_index.vector_stores.duckdb\nimport llama_index.vector_stores.duckdb\n#pip install llama-index-embeddings-openai\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n#pip install llama-index-llms-openai\nfrom llama_index.llms.openai import OpenAI\n#pip install gradio\nimport gradio as gr\n\nThe LLM creates a vector store everytime it runs. So we delete it before storing the new vector\n\nfile_path = 'persist/my_vector_store.duckdb'\n\n# Check if file exists\nif os.path.exists(file_path):\n  #Delete the file\n  os.remove(file_path)\n  print(\"File deleted successfully\")\nelse:\n  print(\"File doesn't exist - first run - it's all good\")\n\nFile deleted successfully\n\n\nNext we find the OPENAI key and set up the environment so that we are able to use its index capability and using llama indexing provided by Meta’s open source\n\nfrom dotenv import load_dotenv\n#load_dotenv()\n\nload_dotenv(dotenv_path=\"secrets/.env\")\n\napi_key = os.getenv('OPENAI_API_KEY')\n\nfrom openai import OpenAI\nclient = OpenAI(api_key=api_key)\n\nHere we import the indexing packages to store the indexing in DuckDB.\n\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.vector_stores.duckdb import DuckDBVectorStore\nfrom llama_index.core import StorageContext\n\nvector_store = DuckDBVectorStore(\"my_vector_store.duckdb\", persist_dir=\"persist/\")\ndocuments = SimpleDirectoryReader(\"/Users/Eileen/Desktop/GoData/Blog/posts/LLM_Demo/OBBBA/\").load_data()\n\nThis is where storage_context points to your indexed PDFs in the storage you specified above\n\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\nindex = VectorStoreIndex.from_documents(documents, storage_context=storage_context)"
  },
  {
    "objectID": "posts/LLM_Demo/index.html#deployment",
    "href": "posts/LLM_Demo/index.html#deployment",
    "title": "Easily Build Customized LLMs",
    "section": "Deployment",
    "text": "Deployment\nThis is a basic interface so that you can query anything about the PDF(s) you stored. In this case we stored two documents from https://www.congress.gov/:\nhttps://www.congress.gov/bill/119th-congress/house-bill/1/text\nhttps://www.congress.gov/bill/119th-congress/house-bill/1\nWe printed them to PDFs and stored them under the OBBBA directory.\nSome responses will not be what you expect and even provide inaccurate in incomplete responses. The reponses have limited output, so it’s best to narrow your questions as much as possible and to provide the sections and provisions for verifications. The prompts below have been tested and can be verified with the above documents. The prompt area is live, so have fun and dig in.\nThis is a basic interface so that you can query anything about the PDF(s) stored.\nSuggested prompts for potential stories:\nWhat is OBBBA?\nHow are Medicaid programs affected? List provision or section number for verification. Output in list.\nHow are SNAP programs affected? List provision or section number for verification. Output list.\nWhat are all the environmental reductions, rescinds, and rescissions? List provisions and section number for verification. Output to list.\nWhat are all the environmental increases, and or establishment appropriations (not rescissions)? List provisions and section number for verification. Output to list.\nWhat are all the welfare reductions, and rescissions. List provisions and section number for verification. Output to list.\nWhat are all the law enforcement reductions, and rescissions. List provisions and section number for verification. Output to list.\nWhat are all the law enforcement increases and or appropriations? List provisions and section number for verification. Output to list.\nWhat are all the welfare appropriations, and increases?. List provisions and section number for verification. Output to list.\nWhat is the DOJ fund named ”BIDEN.” What is it used for? list by item. What provision is it under? How much is being appropriated for it?\nHow is FEMA being appropriated? List the provision or section number along with description for verification. Is FEMA still going to be used for disaster recovery?\n\n\n# import gradio as gr\n\n# def greet(query):\n#     query_engine = index.as_query_engine()\n#     response = query_engine.query(query)\n#     return str(response)\n\n# gr.Interface(\n#   fn=greet,\n#   inputs=gr.Textbox(lines=1, placeholder=\"Enter your query here...\",\n#   label=\"Your Query\"),\n#   outputs=gr.Textbox(label=\"Response\")\n#   ).launch(share=True, pwa=True)\n\n\nfrom IPython.display import IFrame\n\n# Create an IFrame object\n# Parameters: url, width, height\niframe = IFrame('https://godata-obbba.hf.space', width=800, height=600)\n# Display the iframe\niframe"
  },
  {
    "objectID": "posts/RSS_Reader/index.html#a-title",
    "href": "posts/RSS_Reader/index.html#a-title",
    "title": "RSS Feeds",
    "section": "A title",
    "text": "A title\n\n\n\n\n\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis posuere ligula sit amet lacinia. Duis dignissim pellentesque magna, rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit, non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim, at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.\n\n\nRSS Readers may not be in fashion but I believe they are on the comeback. Inspired by an article from InfoWorld(Machlis 2022), with some slight modifications, I created an rss feed to list many the research publications. The list of research papers can look overwhelming, but can be refined or filtered using the search boxes.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\nlibrary(DT)\nlibrary(purrr)\nlibrary(stringr)\nlibrary(lubridate)\nlibrary(tidyRSS)\n\n\nflag=0\n\nmy_feed &lt;- \"https://rss.arxiv.org/rss/cs.LG\"\n\nresult &lt;- tryCatch(\n  {\n    my_feed_data &lt;- tidyfeed(my_feed)\n    cat(\"Number of items:\", nrow(my_feed_data), \"\\n\")\n    my_feed_data |&gt; \n      select(feed_pub_date, item_title, item_link, item_description)\n  },\n  error = function(e) {\n    cat(\"Error caught:\", e$message, \"\\n\")\n    NULL  # return NULL on errorde\n  }\n)\n\nGET request successful. Parsing...\n\n\nNumber of items: 198 \n\nif (is.null(result) || nrow(result) == 0) {\n  flag=1\n  cat(\"Sorry, no papers published today\\n\")\n} else {\n  print(result)\n}\n\n# A tibble: 198 × 4\n   feed_pub_date       item_title                     item_link item_description\n   &lt;dttm&gt;              &lt;chr&gt;                          &lt;chr&gt;     &lt;chr&gt;           \n 1 2025-08-11 00:00:00 Diagrams-to-Dynamics (D2D): E… https://… arXiv:2508.0565…\n 2 2025-08-11 00:00:00 A Graph Neural Network Approa… https://… arXiv:2508.0572…\n 3 2025-08-11 00:00:00 Machine Learning-Based Nonlin… https://… arXiv:2508.0577…\n 4 2025-08-11 00:00:00 From Imperfect Signals to Tru… https://… arXiv:2508.0579…\n 5 2025-08-11 00:00:00 Optimal Linear Baseline Model… https://… arXiv:2508.0583…\n 6 2025-08-11 00:00:00 An Effective Approach for Nod… https://… arXiv:2508.0583…\n 7 2025-08-11 00:00:00 A Markov Decision Process Fra… https://… arXiv:2508.0587…\n 8 2025-08-11 00:00:00 The Fourth State: Signed-Zero… https://… arXiv:2508.0590…\n 9 2025-08-11 00:00:00 Dual Signal Decomposition of … https://… arXiv:2508.0591…\n10 2025-08-11 00:00:00 Fast, Convex and Conditioned … https://… arXiv:2508.0592…\n# ℹ 188 more rows\n\n\n\nif (! flag) {\n  my_feed_data_summary &lt;- my_feed_data |&gt;\n    select(item_title, feed_pub_date, item_link,item_description) }\n\n\nif (! flag) {\n#changed item_title to item_desc\nmy_rss_feed &lt;- my_feed_data_summary |&gt; mutate(\n    item_title = str_glue(\"&lt;a target='_blank' title='{item_title}' href='{item_link}' rel='noopener'&gt;{item_title}&lt;/a&gt;\")\n)}\n\n\nif (! flag) {\n  my_rss_feed_table &lt;- my_rss_feed |&gt; select(-item_link)}\n#my_feed_data_summary\n\n\n\nif (! flag) {\n  DT::datatable(my_rss_feed_table, filter = 'top', escape = FALSE, rownames = FALSE,\n    options = list(\n    search = list(regex = TRUE, caseInsensitive = TRUE),  \n    pageLength = 10,\n    lengthMenu = c(10, 25, 50, 100, 200),\n    autowidth = TRUE,\n   columnDefs = list(list(width = '80%', targets = list(2)))\n    )\n  )}"
  },
  {
    "objectID": "posts/RSS_Reader/index_2.html",
    "href": "posts/RSS_Reader/index_2.html",
    "title": "RSS Feeds",
    "section": "",
    "text": "There are more research papers being published than we could ever consume. This RSS Reader gathers the arXiv papers published today and allows us to search through by keywords.\nInspired by an article from InfoWorld (Machlis 2022). Future feads will include more research papers from platforms into a database allowing for a more thorough historical search.\n\n\n\n# setup includes libraries and functions\n\nsource(\"RSS_functions.R\",chdir=T)\n\n\nContent of RSS_functions.R script\n\n# get path to script\npathway &lt;- here::here(\"posts\",\"RSS_Reader\", \"RSS_functions.R\")\n\n# generate output of lines from script\n\nlines &lt;- readLines(pathway, warn=FALSE)\ncat(lines, sep = \"\\n\")\n\n# Get the most recent papers function\n\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(dplyr))\nlibrary(DT)\nlibrary(tidyRSS)\n\nmost_recent &lt;- function(source) {\n  \n  site=source\n  \n  my_feed_data &lt;- tidyfeed(site) |&gt;\n    select(feed_pub_date,item_title, item_link, item_description)\n  \n  my_feed_data_summary &lt;- my_feed_data |&gt;\n    select(item_title, feed_pub_date, item_link,\n           item_description) \n  \n  #changed item_title to item_desc\n  my_rss_feed &lt;- my_feed_data_summary |&gt; mutate(\n    item_title = str_glue(\"&lt;a target='_blank' title='{item_title}' href='{item_link}' rel='noopener'&gt;{item_title}&lt;/a&gt;\")\n  )\n  \n  my_rss_feed_table &lt;- my_rss_feed |&gt; select(-item_link)\n  #my_feed_data_summary\n  \n  \n  return(my_rss_feed_table)  \n}\n\n\ntryCatch &lt;- function(my_rss_feed_table) {\n  result &lt;- paste(nrow(my_rss_feed_table),\"\\n\")\n  print(result)\n}\n\n\nGetting feed for arXiv and checking the number of results. There are no papers on weekends\n\nflag=0 #Set Error flag to False\n\nmy_feed &lt;- \"https://rss.arxiv.org/rss/cs.LG\"\n\n\n# Call functins in RSS_functions.R\ninitial_results &lt;- most_recent(my_feed) # Records Retreived\n\nGET request successful. Parsing...\n\nresult &lt;- tryCatch(initial_results) # Catch error if no results\n\n[1] \"427 \\n\"\n\n#paste(\"tryCatch: \", result) #Shows result of retrieval\n\n# Sets flag = 1 when no records retrieved\n\nif (is.null(result)) {\n  flag=1 # Sets Error flag = True\n  cat(\"Sorry, no papers published today\\n\") \n} else {paste(\"Records Retrieved:\", result)}\n\n[1] \"Records Retrieved: 427 \\n\"\n\n\n\n\nCreating output using DT library\n\nif (! flag) {\n  DT::datatable(initial_results, filter = 'top', escape = FALSE, rownames = FALSE,\n    options = list(\n    search = list(regex = TRUE, caseInsensitive = TRUE),  \n    pageLength = 10,\n    lengthMenu = c(10, 25, 50, 100, 200),\n    autowidth = TRUE,\n   columnDefs = list(list(width = '80%', targets = list(2)))\n    )\n  )}\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nMachlis, Sharon. 2022. “How to Create Your Own RSS Reader with r _ InfoWorld.” InfoWorld, December. https://www.infoworld.com/article/2337738/how-to-create-your-own-rss-reader-with-r.html."
  },
  {
    "objectID": "posts/RSS_Reader/Most_Recent_Papers_2.html",
    "href": "posts/RSS_Reader/Most_Recent_Papers_2.html",
    "title": "Recent arXiv Research Papers (Updated M-F)",
    "section": "",
    "text": "Set up librarys and"
  },
  {
    "objectID": "posts/pyspark/cafc_merge_data.html",
    "href": "posts/pyspark/cafc_merge_data.html",
    "title": "Data Pipelines using Pyspark",
    "section": "",
    "text": "#| echo: true\n%run cafc_quarterly_fraud_data_pipeline.py\n\n# Load necessary libraries\n# install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n# Set the directory where your partitioned CSV files are located\ndata_directory &lt;- \"outputs_2025_Q2/cleaned_cafc\"\ndata_directory\n\n[1] \"outputs_2025_Q2/cleaned_cafc\"\n\n\n\n# Get a list of all CSV file paths\nfile_paths &lt;- list.files(path = data_directory, pattern = \"\\\\.csv$\", all.files = TRUE, full.names = TRUE)\nfile_paths\n\n[1] \"outputs_2025_Q2/cleaned_cafc/part-00000-26c7da18-5d8f-4d23-a755-48883dd69677-c000.csv\"\n[2] \"outputs_2025_Q2/cleaned_cafc/part-00001-26c7da18-5d8f-4d23-a755-48883dd69677-c000.csv\"\n[3] \"outputs_2025_Q2/cleaned_cafc/part-00002-26c7da18-5d8f-4d23-a755-48883dd69677-c000.csv\"\n[4] \"outputs_2025_Q2/cleaned_cafc/part-00003-26c7da18-5d8f-4d23-a755-48883dd69677-c000.csv\"\n[5] \"outputs_2025_Q2/cleaned_cafc/part-00004-26c7da18-5d8f-4d23-a755-48883dd69677-c000.csv\"\n[6] \"outputs_2025_Q2/cleaned_cafc/part-00005-26c7da18-5d8f-4d23-a755-48883dd69677-c000.csv\"\n[7] \"outputs_2025_Q2/cleaned_cafc/part-00006-26c7da18-5d8f-4d23-a755-48883dd69677-c000.csv\"\n[8] \"outputs_2025_Q2/cleaned_cafc/part-00007-26c7da18-5d8f-4d23-a755-48883dd69677-c000.csv\"\n\n\n\n# Read and combine all CSV files into a single data frame\nmerged_df &lt;- file_paths %&gt;%\n  map_dfr(read_csv, show_col_types = FALSE)\n\n\n# View the first few rows of the merged data frame\nhead(merged_df)\n\n# A tibble: 6 × 13\n     id date       complaint_type country province   fraud_cat sol_method gender\n  &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;          &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt; \n1     2 2021-01-02 CAFC Website   Canada  Quebec     Merchand… Internet   Male  \n2     4 2021-01-02 CAFC Website   Canada  British C… Vendor F… Text mess… Male  \n3     5 2021-01-02 NCFRS          Canada  Alberta    Merchand… Internet   Female\n4     6 2021-01-02 CAFC Website   Canada  Ontario    Phishing  Text mess… Male  \n5     8 2021-01-02 CAFC Website   Canada  Saskatche… Merchand… Other/unk… Not A…\n6     9 2021-01-02 CAFC Website   Canada  British C… Job       Internet-… Female\n# ℹ 5 more variables: lang_cor &lt;chr&gt;, age_range &lt;chr&gt;, complaint_subtype &lt;chr&gt;,\n#   num_victims &lt;dbl&gt;, dollar_loss &lt;dbl&gt;"
  },
  {
    "objectID": "posts/pyspark/index.html",
    "href": "posts/pyspark/index.html",
    "title": "Data Pipelines with Pyspark",
    "section": "",
    "text": "Warning - it takes up to a minute to load. Pyspark is an invaluable tool to process datasets in parallel partitions for processing efficiency. The data can be written and merged to another file with many different options in format, such as csv, tsv, json, parquet, and xml.\nThe structured pipeline once written will always be reproducible and easy to maintain.\nThe Canadian Anti-Fraud Centre updates their dataset every quarter and maintains their dataset on the CKAN platform. The CKAN platform allows federal and muncipal governments as well as companies to maintain their catalog of datasets in a consistent and transparent way, whether it’s public or private to all their users.\nIn this exercise, we are going to use the Canadian Anti-Fraud Centre Reporting Dataset on the CKAN platform and do the following:\n1. Extract english and french field names into 2 datasets to represent English and French distinct but replicated datasets (in progress).\n2. Change date type to date and change fields to numeric that we want to aggregate.\n3. Include monthly and yearly aggregate datasets.\n4. Filter out invalid records that have invalid Country names.\n5. Streamlined access to new generated datasets.\n#| echo: true\n%run cafc_quarterly_fraud_data_pipeline.py #Set up on cronjob to run each quarter\nThis outputs the script used in pyspark to clean the data downloaded from:\n\n# get path to script\npathway &lt;- here::here(\"posts\",\"pyspark\",\"cafc_quarterly_fraud_data_pipeline.py\")\n\npathway\n\n[1] \"/Users/Eileen/Desktop/GoData/Blog/posts/pyspark/cafc_quarterly_fraud_data_pipeline.py\"\n\n# generate output of lines from script\n\nlines &lt;- readLines(pathway, warn=FALSE)\ncat(lines, sep = \"\\n\")\n\nimport os\n#import pandas as pd\n#import matplotlib.pyplot as plt\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, regexp_replace, to_date, trim, when, year, month, sum as _sum\n#from datetime import datetime\n#from reportlab.lib.pagesizes import letter\n#from reportlab.lib import colors\n#from reportlab.pdfgen import canvas\n\n# ------------ 1: START SPARK SESSION ------------\nspark = SparkSession.builder.appName(\"CAFC_Quarterly_Refinery_Pipeline\").getOrCreate()\n\n# ------------ 2: READ & CLEAN DATA ------------\ndf = spark.read.option(\"header\", \"true\").option(\"delimiter\", \"\\t\").csv(\"cafc.tsv\")\n\nfrench_cols = [\n    \"Type de plainte recue\", \"Pays\", \"Province/Etat\",\n    \"Categories thematiques sur la fraude et la cybercriminalite\",\n    \"Methode de sollicitation\", \"Genre\",\n    \"Langue de correspondance\", \"Type de plainte\"\n]\ndf = df.drop(*french_cols)\n\nrename_map = {\n    \"Numero d'identification / Number ID\": \"id\",\n    \"Date Received / Date recue\": \"date\",\n    \"Complaint Received Type\": \"complaint_type\",\n    \"Country\": \"country\", \"Province/State\": \"province\",\n    \"Fraud and Cybercrime Thematic Categories\": \"fraud_cat\",\n    \"Solicitation Method\": \"sol_method\",\n    \"Gender\": \"gender\", \"Language of Correspondence\": \"lang_cor\",\n    \"Victim Age Range / Tranche d'age des victimes\": \"age_range\",\n    \"Complaint Type\": \"complaint_subtype\",\n    \"Number of Victims / Nombre de victimes\": \"num_victims\",\n    \"Dollar Loss /pertes financieres\": \"dollar_loss\"\n}\nfor old, new in rename_map.items():\n    if old in df.columns:\n        df = df.withColumnRenamed(old, new)\n\ndf = df.withColumn(\"num_victims\",\n    when(trim(col(\"num_victims\")) == \"\", None).otherwise(col(\"num_victims\").cast(\"integer\")))\ndf = df.withColumn(\"dollar_loss\",\n    when(trim(col(\"dollar_loss\")) == \"\", None)\n    .otherwise(regexp_replace(col(\"dollar_loss\"), \"[$,]\", \"\").cast(\"double\")))\n\ndf = df.withColumn(\"date\", to_date(\"date\", \"yyyy-MM-dd\"))\n\n# clean blank records\ndf = df.filter(df.country != \"Not Specified\") \n    #.show(truncate=False) \n\n# ------------ 3: DETECT QUARTER ------------\nmin_date = df.agg({\"date\": \"min\"}).collect()[0][0]\nmax_date = df.agg({\"date\": \"max\"}).collect()[0][0]\nyear_val = max_date.year\nquarter_val = (max_date.month - 1) // 3 + 1\nlabel = f\"{year_val}_Q{quarter_val}\"\n\nout_dir = f\"outputs_{label}\"\nos.makedirs(out_dir, exist_ok=True)\n\n# ------------ 4: SAVE CLEANED DATA ------------\ndf.write.mode(\"overwrite\").option(\"header\", True).csv(f\"{out_dir}/cleaned_cafc\")\n\n# ------------ 5: SUMMARIES ------------\nmonthly_summary = df.groupBy(year(\"date\").alias(\"year\"), month(\"date\").alias(\"month\")).agg(\n    _sum(\"dollar_loss\").alias(\"total_loss\"),\n    _sum(\"num_victims\").alias(\"total_victims\")\n).orderBy(\"year\", \"month\")\nmonthly_summary.write.mode(\"overwrite\").option(\"header\", True).csv(f\"{out_dir}/monthly_summary\")\n\nyearly_summary = df.groupBy(year(\"date\").alias(\"year\")).agg(\n    _sum(\"dollar_loss\").alias(\"total_loss\"),\n    _sum(\"num_victims\").alias(\"total_victims\")\n).orderBy(\"year\")\nyearly_summary.write.mode(\"overwrite\").option(\"header\", True).csv(f\"{out_dir}/yearly_summary\")\n\n\nprint(f\"CAFC_Quarterly_Refinery_Pipeline script complete: {out_dir}\")\n\n\nRequired Libraries\n\n# Load necessary libraries\n# install.packages(\"tidyverse\")\nsuppressPackageStartupMessages(library(tidyverse))\n\nThis is where the new files will be stored\n\n# Set the directory where your partitioned CSV files are located\ndata_directory &lt;- \"outputs_2025_Q2/cleaned_cafc\"\ndata_directory\n\n[1] \"outputs_2025_Q2/cleaned_cafc\"\n\n\nWhen files are outputed there will be several files from different partitions\n\n# Get a list of all CSV file paths\nfile_paths &lt;- list.files(path = data_directory, pattern = \"\\\\.csv$\", all.files = TRUE, full.names = TRUE)\nfile_paths\n\n[1] \"outputs_2025_Q2/cleaned_cafc/part-00000-b9ce166a-69b6-41ae-b68c-17dc6f4c3e5f-c000.csv\"\n[2] \"outputs_2025_Q2/cleaned_cafc/part-00001-b9ce166a-69b6-41ae-b68c-17dc6f4c3e5f-c000.csv\"\n[3] \"outputs_2025_Q2/cleaned_cafc/part-00002-b9ce166a-69b6-41ae-b68c-17dc6f4c3e5f-c000.csv\"\n[4] \"outputs_2025_Q2/cleaned_cafc/part-00003-b9ce166a-69b6-41ae-b68c-17dc6f4c3e5f-c000.csv\"\n[5] \"outputs_2025_Q2/cleaned_cafc/part-00004-b9ce166a-69b6-41ae-b68c-17dc6f4c3e5f-c000.csv\"\n[6] \"outputs_2025_Q2/cleaned_cafc/part-00005-b9ce166a-69b6-41ae-b68c-17dc6f4c3e5f-c000.csv\"\n[7] \"outputs_2025_Q2/cleaned_cafc/part-00006-b9ce166a-69b6-41ae-b68c-17dc6f4c3e5f-c000.csv\"\n[8] \"outputs_2025_Q2/cleaned_cafc/part-00007-b9ce166a-69b6-41ae-b68c-17dc6f4c3e5f-c000.csv\"\n\n\nMerge the partitions to one file\n\n# Read and combine all CSV files into a single data frame\nmerged_df &lt;- file_paths %&gt;%\n  map_dfr(read_csv, show_col_types = FALSE)\n\nShow records of merged file\n\n# View the first few rows of the merged data frame\nhead(merged_df)\n\n# A tibble: 6 × 13\n     id date       complaint_type country province   fraud_cat sol_method gender\n  &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;          &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt; \n1     2 2021-01-02 CAFC Website   Canada  Quebec     Merchand… Internet   Male  \n2     4 2021-01-02 CAFC Website   Canada  British C… Vendor F… Text mess… Male  \n3     5 2021-01-02 NCFRS          Canada  Alberta    Merchand… Internet   Female\n4     6 2021-01-02 CAFC Website   Canada  Ontario    Phishing  Text mess… Male  \n5     8 2021-01-02 CAFC Website   Canada  Saskatche… Merchand… Other/unk… Not A…\n6     9 2021-01-02 CAFC Website   Canada  British C… Job       Internet-… Female\n# ℹ 5 more variables: lang_cor &lt;chr&gt;, age_range &lt;chr&gt;, complaint_subtype &lt;chr&gt;,\n#   num_victims &lt;dbl&gt;, dollar_loss &lt;dbl&gt;\n\n\n\ndata_directory &lt;- \"outputs_2025_Q2/monthly_summary\"\nfile_paths &lt;- list.files(path = data_directory, pattern = \"\\\\.csv$\", all.files = TRUE, full.names = TRUE)\nfile_paths\n\n[1] \"outputs_2025_Q2/monthly_summary/part-00000-bb97576f-ce4a-400a-91ef-d8af632d0b57-c000.csv\"\n\nmonthly_summary_df &lt;- file_paths %&gt;%\n  map_dfr(read_csv, show_col_types = FALSE)\n\n\nhead(monthly_summary_df)\n\n# A tibble: 6 × 4\n   year month total_loss total_victims\n  &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;\n1  2021     1  12894120.          4411\n2  2021     2  27042043.          5055\n3  2021     3  17017938.          6111\n4  2021     4  22120277.          4711\n5  2021     5  19401052.          4348\n6  2021     6  19796861.          4633\n\n\n\ndata_directory &lt;- \"outputs_2025_Q2/yearly_summary\"\nfile_paths &lt;- list.files(path = data_directory, pattern = \"\\\\.csv$\", all.files = TRUE, full.names = TRUE)\nfile_paths\n\n[1] \"outputs_2025_Q2/yearly_summary/part-00000-1ce6d64d-3176-4cbd-8fb9-7dd33486ba65-c000.csv\"\n\nyearly_summary_df &lt;- file_paths %&gt;%\n  map_dfr(read_csv, show_col_types = FALSE)\n\n\nhead(yearly_summary_df)\n\n# A tibble: 5 × 3\n   year total_loss total_victims\n  &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;\n1  2021 309470307.         52830\n2  2022 444393290.         47537\n3  2023 497598850.         35714\n4  2024 527081208.         30241\n5  2025 270106027.         14181"
  },
  {
    "objectID": "posts/pyspark/index.html#introduction",
    "href": "posts/pyspark/index.html#introduction",
    "title": "Data Pipelines with Pyspark",
    "section": "",
    "text": "Warning - it takes up to a minute to load. Pyspark is an invaluable tool to process datasets in parallel partitions for processing efficiency. The data can be written and merged to another file with many different options in format, such as csv, tsv, json, parquet, and xml.\nThe structured pipeline once written will always be reproducible and easy to maintain.\nThe Canadian Anti-Fraud Centre updates their dataset every quarter and maintains their dataset on the CKAN platform. The CKAN platform allows federal and muncipal governments as well as companies to maintain their catalog of datasets in a consistent and transparent way, whether it’s public or private to all their users.\nIn this exercise, we are going to use the Canadian Anti-Fraud Centre Reporting Dataset on the CKAN platform and do the following:\n1. Extract english and french field names into 2 datasets to represent English and French distinct but replicated datasets (in progress).\n2. Change date type to date and change fields to numeric that we want to aggregate.\n3. Include monthly and yearly aggregate datasets.\n4. Filter out invalid records that have invalid Country names.\n5. Streamlined access to new generated datasets.\n#| echo: true\n%run cafc_quarterly_fraud_data_pipeline.py #Set up on cronjob to run each quarter\nThis outputs the script used in pyspark to clean the data downloaded from:\n\n# get path to script\npathway &lt;- here::here(\"posts\",\"pyspark\",\"cafc_quarterly_fraud_data_pipeline.py\")\n\npathway\n\n[1] \"/Users/Eileen/Desktop/GoData/Blog/posts/pyspark/cafc_quarterly_fraud_data_pipeline.py\"\n\n# generate output of lines from script\n\nlines &lt;- readLines(pathway, warn=FALSE)\ncat(lines, sep = \"\\n\")\n\nimport os\n#import pandas as pd\n#import matplotlib.pyplot as plt\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, regexp_replace, to_date, trim, when, year, month, sum as _sum\n#from datetime import datetime\n#from reportlab.lib.pagesizes import letter\n#from reportlab.lib import colors\n#from reportlab.pdfgen import canvas\n\n# ------------ 1: START SPARK SESSION ------------\nspark = SparkSession.builder.appName(\"CAFC_Quarterly_Refinery_Pipeline\").getOrCreate()\n\n# ------------ 2: READ & CLEAN DATA ------------\ndf = spark.read.option(\"header\", \"true\").option(\"delimiter\", \"\\t\").csv(\"cafc.tsv\")\n\nfrench_cols = [\n    \"Type de plainte recue\", \"Pays\", \"Province/Etat\",\n    \"Categories thematiques sur la fraude et la cybercriminalite\",\n    \"Methode de sollicitation\", \"Genre\",\n    \"Langue de correspondance\", \"Type de plainte\"\n]\ndf = df.drop(*french_cols)\n\nrename_map = {\n    \"Numero d'identification / Number ID\": \"id\",\n    \"Date Received / Date recue\": \"date\",\n    \"Complaint Received Type\": \"complaint_type\",\n    \"Country\": \"country\", \"Province/State\": \"province\",\n    \"Fraud and Cybercrime Thematic Categories\": \"fraud_cat\",\n    \"Solicitation Method\": \"sol_method\",\n    \"Gender\": \"gender\", \"Language of Correspondence\": \"lang_cor\",\n    \"Victim Age Range / Tranche d'age des victimes\": \"age_range\",\n    \"Complaint Type\": \"complaint_subtype\",\n    \"Number of Victims / Nombre de victimes\": \"num_victims\",\n    \"Dollar Loss /pertes financieres\": \"dollar_loss\"\n}\nfor old, new in rename_map.items():\n    if old in df.columns:\n        df = df.withColumnRenamed(old, new)\n\ndf = df.withColumn(\"num_victims\",\n    when(trim(col(\"num_victims\")) == \"\", None).otherwise(col(\"num_victims\").cast(\"integer\")))\ndf = df.withColumn(\"dollar_loss\",\n    when(trim(col(\"dollar_loss\")) == \"\", None)\n    .otherwise(regexp_replace(col(\"dollar_loss\"), \"[$,]\", \"\").cast(\"double\")))\n\ndf = df.withColumn(\"date\", to_date(\"date\", \"yyyy-MM-dd\"))\n\n# clean blank records\ndf = df.filter(df.country != \"Not Specified\") \n    #.show(truncate=False) \n\n# ------------ 3: DETECT QUARTER ------------\nmin_date = df.agg({\"date\": \"min\"}).collect()[0][0]\nmax_date = df.agg({\"date\": \"max\"}).collect()[0][0]\nyear_val = max_date.year\nquarter_val = (max_date.month - 1) // 3 + 1\nlabel = f\"{year_val}_Q{quarter_val}\"\n\nout_dir = f\"outputs_{label}\"\nos.makedirs(out_dir, exist_ok=True)\n\n# ------------ 4: SAVE CLEANED DATA ------------\ndf.write.mode(\"overwrite\").option(\"header\", True).csv(f\"{out_dir}/cleaned_cafc\")\n\n# ------------ 5: SUMMARIES ------------\nmonthly_summary = df.groupBy(year(\"date\").alias(\"year\"), month(\"date\").alias(\"month\")).agg(\n    _sum(\"dollar_loss\").alias(\"total_loss\"),\n    _sum(\"num_victims\").alias(\"total_victims\")\n).orderBy(\"year\", \"month\")\nmonthly_summary.write.mode(\"overwrite\").option(\"header\", True).csv(f\"{out_dir}/monthly_summary\")\n\nyearly_summary = df.groupBy(year(\"date\").alias(\"year\")).agg(\n    _sum(\"dollar_loss\").alias(\"total_loss\"),\n    _sum(\"num_victims\").alias(\"total_victims\")\n).orderBy(\"year\")\nyearly_summary.write.mode(\"overwrite\").option(\"header\", True).csv(f\"{out_dir}/yearly_summary\")\n\n\nprint(f\"CAFC_Quarterly_Refinery_Pipeline script complete: {out_dir}\")\n\n\nRequired Libraries\n\n# Load necessary libraries\n# install.packages(\"tidyverse\")\nsuppressPackageStartupMessages(library(tidyverse))\n\nThis is where the new files will be stored\n\n# Set the directory where your partitioned CSV files are located\ndata_directory &lt;- \"outputs_2025_Q2/cleaned_cafc\"\ndata_directory\n\n[1] \"outputs_2025_Q2/cleaned_cafc\"\n\n\nWhen files are outputed there will be several files from different partitions\n\n# Get a list of all CSV file paths\nfile_paths &lt;- list.files(path = data_directory, pattern = \"\\\\.csv$\", all.files = TRUE, full.names = TRUE)\nfile_paths\n\n[1] \"outputs_2025_Q2/cleaned_cafc/part-00000-b9ce166a-69b6-41ae-b68c-17dc6f4c3e5f-c000.csv\"\n[2] \"outputs_2025_Q2/cleaned_cafc/part-00001-b9ce166a-69b6-41ae-b68c-17dc6f4c3e5f-c000.csv\"\n[3] \"outputs_2025_Q2/cleaned_cafc/part-00002-b9ce166a-69b6-41ae-b68c-17dc6f4c3e5f-c000.csv\"\n[4] \"outputs_2025_Q2/cleaned_cafc/part-00003-b9ce166a-69b6-41ae-b68c-17dc6f4c3e5f-c000.csv\"\n[5] \"outputs_2025_Q2/cleaned_cafc/part-00004-b9ce166a-69b6-41ae-b68c-17dc6f4c3e5f-c000.csv\"\n[6] \"outputs_2025_Q2/cleaned_cafc/part-00005-b9ce166a-69b6-41ae-b68c-17dc6f4c3e5f-c000.csv\"\n[7] \"outputs_2025_Q2/cleaned_cafc/part-00006-b9ce166a-69b6-41ae-b68c-17dc6f4c3e5f-c000.csv\"\n[8] \"outputs_2025_Q2/cleaned_cafc/part-00007-b9ce166a-69b6-41ae-b68c-17dc6f4c3e5f-c000.csv\"\n\n\nMerge the partitions to one file\n\n# Read and combine all CSV files into a single data frame\nmerged_df &lt;- file_paths %&gt;%\n  map_dfr(read_csv, show_col_types = FALSE)\n\nShow records of merged file\n\n# View the first few rows of the merged data frame\nhead(merged_df)\n\n# A tibble: 6 × 13\n     id date       complaint_type country province   fraud_cat sol_method gender\n  &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;          &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt; \n1     2 2021-01-02 CAFC Website   Canada  Quebec     Merchand… Internet   Male  \n2     4 2021-01-02 CAFC Website   Canada  British C… Vendor F… Text mess… Male  \n3     5 2021-01-02 NCFRS          Canada  Alberta    Merchand… Internet   Female\n4     6 2021-01-02 CAFC Website   Canada  Ontario    Phishing  Text mess… Male  \n5     8 2021-01-02 CAFC Website   Canada  Saskatche… Merchand… Other/unk… Not A…\n6     9 2021-01-02 CAFC Website   Canada  British C… Job       Internet-… Female\n# ℹ 5 more variables: lang_cor &lt;chr&gt;, age_range &lt;chr&gt;, complaint_subtype &lt;chr&gt;,\n#   num_victims &lt;dbl&gt;, dollar_loss &lt;dbl&gt;\n\n\n\ndata_directory &lt;- \"outputs_2025_Q2/monthly_summary\"\nfile_paths &lt;- list.files(path = data_directory, pattern = \"\\\\.csv$\", all.files = TRUE, full.names = TRUE)\nfile_paths\n\n[1] \"outputs_2025_Q2/monthly_summary/part-00000-bb97576f-ce4a-400a-91ef-d8af632d0b57-c000.csv\"\n\nmonthly_summary_df &lt;- file_paths %&gt;%\n  map_dfr(read_csv, show_col_types = FALSE)\n\n\nhead(monthly_summary_df)\n\n# A tibble: 6 × 4\n   year month total_loss total_victims\n  &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;\n1  2021     1  12894120.          4411\n2  2021     2  27042043.          5055\n3  2021     3  17017938.          6111\n4  2021     4  22120277.          4711\n5  2021     5  19401052.          4348\n6  2021     6  19796861.          4633\n\n\n\ndata_directory &lt;- \"outputs_2025_Q2/yearly_summary\"\nfile_paths &lt;- list.files(path = data_directory, pattern = \"\\\\.csv$\", all.files = TRUE, full.names = TRUE)\nfile_paths\n\n[1] \"outputs_2025_Q2/yearly_summary/part-00000-1ce6d64d-3176-4cbd-8fb9-7dd33486ba65-c000.csv\"\n\nyearly_summary_df &lt;- file_paths %&gt;%\n  map_dfr(read_csv, show_col_types = FALSE)\n\n\nhead(yearly_summary_df)\n\n# A tibble: 5 × 3\n   year total_loss total_victims\n  &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;\n1  2021 309470307.         52830\n2  2022 444393290.         47537\n3  2023 497598850.         35714\n4  2024 527081208.         30241\n5  2025 270106027.         14181"
  },
  {
    "objectID": "posts/pyspark/index.html#deployment",
    "href": "posts/pyspark/index.html#deployment",
    "title": "Data Pipelines with Pyspark",
    "section": "Deployment",
    "text": "Deployment\n\nlibrary(downloadthis)\n\nmerged_df %&gt;%\n  download_this(\n    output_name = \"clean_cafc_en.csv\",\n    output_extension = \".csv\",\n    button_label = \"Download clean_cafc_en.csv\",\n    button_type = \"default\",\n    self_contained = TRUE,\n    has_icon = TRUE,\n    icon = \"fa fa-save\",\n    id = \"cafc-btn\"\n  )\n\n Download clean_cafc_en.csv\n\n\n\nlibrary(downloadthis)\n\nmonthly_summary_df %&gt;%\n  download_this(\n    output_name = \"cafc_monthly_summary_en\",\n    output_extension = \".csv\",\n    button_label = \"Download cafc_monthly_summary_en.csv\",\n    button_type = \"default\",\n    self_contained = TRUE,\n    has_icon = TRUE,\n    icon = \"fa fa-save\",\n    id = \"cafc-btn\"\n  )\n\n Download cafc_monthly_summary_en.csv\n\n\n\nlibrary(downloadthis)\n\nyearly_summary_df %&gt;%\n  download_this(\n    output_name = \"cafc_yearly_summary_en\",\n    output_extension = \".csv\",\n    button_label = \"Download cafc_yearly_summary_en.csv\",\n    button_type = \"default\",\n    self_contained = TRUE,\n    has_icon = TRUE,\n    icon = \"fa fa-save\",\n    id = \"cafc-btn\"\n  )\n\n Download cafc_yearly_summary_en.csv"
  },
  {
    "objectID": "posts/test/testplot.html",
    "href": "posts/test/testplot.html",
    "title": "Untitled",
    "section": "",
    "text": "library(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n# Paste your data as a string\ncsv_data &lt;- \"year;month;total_loss;total_victims\n2021;1;12894119,87;4411\n2021;2;27042043,27;5055\n2021;3;17017937,67;6111\n2021;4;22120277,22;4711\n2021;5;19401051,74;4348\n2021;6;19796860,95;4633\n2021;7;21723251,47;4525\n2021;8;21036153,60;4211\n2021;9;28828754,70;3739\n2021;10;33303225,59;3660\n2021;11;41681068,45;3672\n2021;12;44625562,72;3754\n2022;1;31814962,40;4041\n2022;2;36026119,84;3299\n2022;3;43628771,24;4432\n2022;4;36476311,04;4577\n2022;5;38666778,10;4548\n2022;6;30129928,36;4157\n2022;7;28081496,50;3854\n2022;8;31819633,63;3954\n2022;9;31264147,37;3524\n2022;10;49055187,23;3361\n2022;11;54803160,95;3849\n2022;12;32626793,71;3941\n2023;1;37670116,02;3522\n2023;2;43413776,54;3179\n2023;3;41137878,02;3680\n2023;4;33922844,57;1843\n2023;5;51638152,13;3136\n2023;6;48023733,31;3280\n2023;7;32043429,73;3192\n2023;8;43296721,84;2906\n2023;9;43249536,17;2902\n2023;10;41146026,32;2839\n2023;11;39028500,22;2528\n2023;12;43028135,00;2707\n2024;1;34450108,75;2657\n2024;2;34808929,03;2481\n2024;3;31594065,16;2241\n2024;4;59771349,65;2658\n2024;5;34796328,01;2653\n2024;6;52479479,07;2370\n2024;7;40049224,25;2317\n2024;8;33966270,42;2704\n2024;9;48044584,96;2565\n2024;10;50380513,42;2568\n2024;11;61345205,55;2513\n2024;12;45395149,66;2514\n2025;1;34082138,32;2574\n2025;2;43393669,67;2278\n2025;3;52804058,12;2869\n2025;4;51230348,80;2400\n2025;5;44634331,50;2225\n2025;6;43961480,63;1835\"\n\n# Read the data\ndata &lt;- read_delim(csv_data, delim = \";\", col_types='iidn')\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\ndata$total_loss &lt;- as.numeric(gsub(\",\", \".\", data$total_loss))\n\n# Create a date variable (first day of each month)\ndata &lt;- data %&gt;%\n  mutate(date = ymd(sprintf(\"%d-%02d-01\", year, month)))\n\n# Plot\nggplot(data, aes(x = date, y = total_loss)) +\n  geom_line(color = \"blue\", size = 1) +\n  scale_x_date(date_labels = \"%Y-%m\", date_breaks = \"3 months\") +\n  labs(title = \"Monthly Total Losses Reported to CAFC\", x = \"Year-Month\", y = \"Total Loss ($)\") +\n  theme_minimal() +\n  theme(axis.text.x=element_text(angle=45, hjust=1))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: Removed 54 rows containing missing values or values outside the scale range\n(`geom_line()`)."
  }
]